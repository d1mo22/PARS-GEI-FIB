{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Lab: Aprendizaje por refuerzo multi-agente (III)\n",
    "\n",
    "En el notebook 08 sobre JAL-GT se presenta al final el algoritmo JAL-AM que propone una solución al potencial problema de no disponer de acceso a la señal de recompensa de los otros agentes. En este caso, los métodos de modelado de agentes (_Agent Modelling, AM_) se pueden utilizar para inferir la política de los demás agentes via observación externa. JAL-AM garantiza convergencia a estrategias óptimas de _best response_ (mejor respuesta).\n",
    "\n",
    "En determinados entornos, es posible que tampoco tengamos acceso a las observaciones sobre las acciones que los demás agentes están realizando sobre el entorno. En este notebook, vamos a explorar un método de gradiente de política que garantiza convergencia a equilibrio de Nash sin necesidad de tener acceso a señal de recompensa ni acciones realizadas de los demás agentes.\n",
    "\n",
    "El método se llama WoLF-PHC: _Win or Learn Fast - Policy Hill Climbing_ y se puede consultar su diseño y su motivación a partir de la página 145 del libro [Multi-Agent Reinforcement Learning, Albrecht et al. 2024](https://www.marl-book.com/download/). La idea principal del algoritmo consiste en construir y actualizar tres elementos:\n",
    "\n",
    "* El valor Q para el agente que entrena (como en Q-Learning, JAL-GT y JAL-AM).\n",
    "* La política $\\pi$ que se está aprendiendo.\n",
    "* La política _media_ $\\bar{\\pi}$, que es una media de los valores más recientes de la política.\n",
    "\n",
    "El motivo de tener, por separado, la política que se aprende y una media de las últimas políticas es que, para una parte substancial de juegos en forma normal y extensiva, esta política media converge en el infinito en un equilibrio de Nash del agente. Intuitivamente: esta política media contiene suficiente información temporal como para reflejar la dinámica de la convergencia hacia el equilibrio basándose en las recompensas de nuestro agente y de los cambios en las estrategias de los otros agentes. Más información sobre los fundamentos teóricos de estas propiedades se pueden encontrar en las Secciones 6.3.1 y 6.4.3 de [Multi-Agent Reinforcement Learning, Albrecht et al. 2024](https://www.marl-book.com/download/).\n",
    "\n",
    "En cada iteración, actualizamos el valor Q (como en otros algoritmos ya vistos) y la política media. Después, se comprueba si el valor esperado actual (multiplicando política por valor Q) mejora o empeora a la política media. Se pueden dar dos casos:\n",
    "\n",
    "* Mejoramos a la política media: en este caso, el gradiente de la política será pequeño (_loss learning rate_ o $lr_l$.\n",
    "* No mejoramos a la política media: el gradiente será grande (_win learning rate_ o $lr_w$).\n",
    "\n",
    "Esta es la parte a la que hace referencia el nombre del algoritmo: _win or learn fast_. Si estamos mejorando, modificamos la política muy poco y somos conservadores. Si no estamos mejorando, buscamos rápidamente un cambio a mejores utilidades esperadas, modificando sustancialmente la política. Este tipo de búsqueda se asemeja a _hill climbing_, y de ahí el sufijo PHC en el nombre.\n",
    "\n",
    "El gradiente, una vez decidido si es el _loss_ o el _win learning rate_, se aplica sobre la acción con mejor valor Q, de manera positiva, y sobre las demás acciones, de manera negativa.\n",
    "\n",
    "El pseudocódigo es el siguiente:\n",
    "\n",
    "<div>\n",
    "<img src=\"Algorithm9.png\" width=\"500\"/>\n",
    "</div>\n",
    "\n",
    "<div>\n",
    "<img src=\"Algorithm9-eq.png\" width=\"500\"/>\n",
    "</div>\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Instalación y configuración"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Dependencias"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!pip --quiet install rlcard pettingzoo seaborn matplotlib numpy pandas tinynn pygame"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Imports necesarios"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import abc\n",
    "import itertools\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "from pettingzoo.classic import rps_v2\n",
    "from tinynn.core.layer import Dense, ReLU\n",
    "from tinynn.core.loss import MSE\n",
    "from tinynn.core.initializer import Zeros\n",
    "from tinynn.core.model import Model\n",
    "from tinynn.core.net import Net\n",
    "from tinynn.core.optimizer import SGD"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9051a731dbcc198b",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Código de anteriores notebooks"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "RPS_CHOICES = [\"Rock\", \"Paper\", \"Scissors\"]\n",
    "PD_CHOICES = [\"Cooperate\", \"Defect\"]\n",
    "\n",
    "def pretty_print_array(ar):\n",
    "    return np.array_str(np.array(ar), precision=2, suppress_small=True)\n",
    "\n",
    "def draw_history(history, title):\n",
    "    data = pd.DataFrame({'Episode': range(1, len(history) + 1), title: history})\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.lineplot(x='Episode', y=title, data=data)\n",
    "\n",
    "    plt.title(title + ' Over Episodes')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel(title)\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d67aed76b8676ff",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class GameModel:\n",
    "    def __init__(self, num_agents, num_states, num_actions):\n",
    "        self.num_agents = num_agents\n",
    "        self.num_states = num_states\n",
    "        self.num_actions = num_actions\n",
    "        self.action_space = self.generate_action_space()\n",
    "        self.action_space_index = {joint_action: idx for idx, joint_action in enumerate(self.action_space)}\n",
    "\n",
    "    def generate_action_space(self):\n",
    "        actions_by_players = []\n",
    "        for agent_id in range(self.num_agents):\n",
    "            actions_by_players.append(range(self.num_actions))\n",
    "        all_joint_actions = itertools.product(*actions_by_players)\n",
    "        return [tuple(l) for l in all_joint_actions]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4bf883b79b33ef3c",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class MARLAlgorithm(abc.ABC):\n",
    "    @abc.abstractmethod\n",
    "    def learn(self, joint_action, rewards, state, next_state):\n",
    "        pass\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def select_action(self, state):\n",
    "        pass"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "728e1e54493bc98b",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def one_hot(state, max_states):\n",
    "    one_hot_vector = np.zeros(max_states)\n",
    "    one_hot_vector[state] = 1\n",
    "    return np.array([one_hot_vector], dtype=float)\n",
    "\n",
    "def normal_form_obs_to_state(observation):\n",
    "    # Sólo un estado (juego en forma normal)\n",
    "    return 0\n",
    "\n",
    "def train(env, game, f_obs_to_state, algorithms):\n",
    "    cumulative_rewards = [[0, 0]]\n",
    "    actions_played = [[], []]\n",
    "    all_agents = range(game.num_agents)\n",
    "\n",
    "    observations, _ = env.reset()\n",
    "    states = [f_obs_to_state(observations[f\"player_{i}\"]) for i in all_agents]\n",
    "\n",
    "    while env.agents:\n",
    "        joint_action = tuple([algorithms[i].select_action(states[i]) for i in all_agents])\n",
    "        [actions_played[i].append(joint_action[i]) for i in all_agents]\n",
    "        pettingzoo_joint_action = {f\"player_{i}\": joint_action[i] for i in all_agents}\n",
    "\n",
    "        observations, rewards, terminations, truncations, infos = env.step(pettingzoo_joint_action)\n",
    "\n",
    "        observations = [observations[f\"player_{i}\"] for i in all_agents]\n",
    "        rewards = [rewards[f\"player_{i}\"] for i in all_agents]\n",
    "        new_states = [f_obs_to_state(observations[i]) for i in all_agents]\n",
    "        [algorithms[i].learn(joint_action, rewards, states[i], new_states[i])\n",
    "         for i in all_agents]\n",
    "        cumulative_rewards.append([cumulative_rewards[-1][i] + rewards[i] for i in all_agents])\n",
    "        states = new_states\n",
    "\n",
    "    return cumulative_rewards, actions_played"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7fb59accc412f315",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Implementación del algoritmo WoLF-PHC"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "En primer lugar definimos una función de normalización para convertir cualquier lista de números reales en una distribución de probabilidad con suma total 1. Esta conversión es lineal, en comparación a softmax que hace una conversión exponencial."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def normalize(x):\n",
    "    min_val = np.min(x)\n",
    "    if min_val < 0:\n",
    "        x -= min_val\n",
    "    total_sum = np.sum(x)\n",
    "    if total_sum == 0:\n",
    "        return [1/len(x)] * len(x)\n",
    "    normalized_probabilities = np.array([p/total_sum for p in x])\n",
    "    return normalized_probabilities"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Ahora definimos la clase que implementa el algoritmo. Observad cómo usamos tres redes neuronales para poder capturar espacios de estados potencialmente complejos: una red para el valor Q, otra para la política, y otra para la política media:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class WoLFPHC(MARLAlgorithm):\n",
    "    def __init__(self, agent_id, game: GameModel, learning_rate_loss, learning_rate_win, gamma=0.95, alpha=0.5, epsilon=0.2, seed=42):\n",
    "        assert(learning_rate_loss > learning_rate_win)\n",
    "        self.agent_id = agent_id\n",
    "        self.game = game\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.learning_rate_win = learning_rate_win\n",
    "        self.learning_rate_loss = learning_rate_loss\n",
    "        self.rng = random.Random(seed)\n",
    "        net_q = Net([Dense(64, w_init=Zeros(), b_init=Zeros()),\n",
    "                     ReLU(),\n",
    "                     Dense(32, w_init=Zeros(), b_init=Zeros()),\n",
    "                     ReLU(),\n",
    "                     Dense(game.num_actions, w_init=Zeros(), b_init=Zeros())])\n",
    "        self.q_model = Model(net=net_q, loss=MSE(), optimizer=SGD(lr=alpha))\n",
    "        self.visit_counts = np.zeros(game.num_states)\n",
    "        net_pi = Net([Dense(64, w_init=Zeros(), b_init=Zeros()),\n",
    "                      ReLU(),\n",
    "                      Dense(32, w_init=Zeros(), b_init=Zeros()),\n",
    "                      ReLU(),\n",
    "                      Dense(game.num_actions, w_init=Zeros(), b_init=Zeros())])\n",
    "        self.policy = Model(net=net_pi, loss=MSE(), optimizer=SGD(lr=0.1))\n",
    "        net_avg_pi = Net([Dense(64, w_init=Zeros(), b_init=Zeros()),\n",
    "                          ReLU(),\n",
    "                          Dense(32, w_init=Zeros(), b_init=Zeros()),\n",
    "                          ReLU(),\n",
    "                          Dense(game.num_actions, w_init=Zeros(), b_init=Zeros())])\n",
    "        self.average_policy = Model(net=net_avg_pi, loss=MSE(), optimizer=SGD(lr=0.1))\n",
    "        self.metrics = {'td_error': [], 'loss': []}"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "14e9eaed5e1a488b",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "El `learning rate` de las redes de política y política media debería ser diferente de la tasa de aprendizaje para el valor Q, debido a que los gradientes de política en este algoritmo son fijos. Por lo tanto, podemos dejar estas dos tasas prefijadas y controlar la convergencia del algoritmo por las dos tasas de aprendizaje $lr_l$ y $lr_w$."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "A continuación añadimos el método para actualizar el valor Q, que es idéntico al visto en el notebook anterior para JAL-GT:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class WoLFPHC(WoLFPHC):\n",
    "    def update_q(self, q_model, state, action, reward, next_value):\n",
    "        prediction = q_model.forward(state)[0]\n",
    "        agent_q_value = prediction[action]\n",
    "        td_target = reward + self.gamma * next_value\n",
    "        td_error = td_target - agent_q_value\n",
    "        target = prediction.copy()\n",
    "        target[action] = td_target\n",
    "        loss, grads = q_model.backward(np.array([prediction]), np.array([target]))\n",
    "        q_model.apply_grads(grads)\n",
    "        return td_error, loss"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "El siguiente paso según el pseudocódigo es actualizar la política media, que en el infinito debería converger en una estrategia de equilibrio de Nash. Observad que al estar tratando con distribuciones de probabilidad (lo que es una política) necesitamos normalizar en determinados puntos clave:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class WoLFPHC(WoLFPHC):\n",
    "    def update_average_policy(self, scalar_state):\n",
    "        state = one_hot(scalar_state, self.game.num_states)\n",
    "        avg_pi_s = self.average_policy.forward(state)[0]\n",
    "        current_pi_s = normalize(self.policy.forward(state)[0])\n",
    "        target_avg_pi_s = avg_pi_s.copy()\n",
    "        for a_i in range(self.game.num_actions):\n",
    "            pi_a_i = current_pi_s[a_i]\n",
    "            # Sumamos el gradiente del error: diferencia entre política actual y política media,\n",
    "            # ponderado por el número de visitas en el estado para converger en un valor estable\n",
    "            target_avg_pi_s[a_i] += (pi_a_i - normalize(avg_pi_s)[a_i]) / self.visit_counts[scalar_state]\n",
    "        target_avg_pi_s = normalize(target_avg_pi_s)\n",
    "        # Actualizamos la red neuronal\n",
    "        _, grads = self.average_policy.backward(np.array([avg_pi_s]), np.array([target_avg_pi_s]))\n",
    "        self.average_policy.apply_grads(grads)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Después de actualizar la política media toca actualizar la política que se está aprendiendo. En este caso juntamos las ecuaciones 6.44 a 6.46:\n",
    "\n",
    "* Calculamos la utilidad esperada en el estado, para la política y para la política media (multiplicación de Q por cada distribución de probabilidad).\n",
    "* Dependiendo de si estamos ganando o no a la política media, decidimos una tasa de aprendizaje que será nuestro gradiente.\n",
    "* Aplicamos el gradiente a cada acción, positivamente a las mejores acciones, negativamente a las demás.\n",
    "* Actualizamos la red neuronal en base a estos gradientes."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class WoLFPHC(WoLFPHC):\n",
    "    def update_policy(self, scalar_state):\n",
    "        state = one_hot(scalar_state, self.game.num_states)\n",
    "\n",
    "        current_policy_s = self.policy.forward(state)[0]\n",
    "        current_avg_pi_s = self.average_policy.forward(state)[0]\n",
    "        q_values_s = self.q_model.forward(state)[0]\n",
    "\n",
    "        # Calculamos delta (ecuación 6.46)\n",
    "        policy_value = np.sum(np.multiply(normalize(current_policy_s), q_values_s))\n",
    "        avg_pi_value = np.sum(np.multiply(normalize(current_avg_pi_s), q_values_s))\n",
    "        if policy_value > avg_pi_value:\n",
    "            delta = self.learning_rate_win\n",
    "        else:\n",
    "            delta = self.learning_rate_loss\n",
    "\n",
    "        # Calculamos delta para (s, a_i) (ecuación 6.45)\n",
    "        delta_all = [min(normalize(current_policy_s)[a_i], delta / (self.game.num_actions - 1)) for a_i in range(self.game.num_actions)]\n",
    "\n",
    "        # Comprobamos qué acción es la mejor según Q y con qué valor\n",
    "        q_values_state = self.q_model.forward(state)[0]\n",
    "        max_q = np.max(q_values_state)\n",
    "        target_policy_s = current_policy_s.copy()\n",
    "\n",
    "        # Recorremos las acciones, aplicando delta\n",
    "        for a_i in range(self.game.num_actions):\n",
    "            if q_values_state[a_i] == max_q:\n",
    "                # Mejores acciones, aplicamos positivamente\n",
    "                target_policy_s[a_i] += sum([delta_all[a_ii] if a_ii != a_i else 0 for a_ii in range(self.game.num_actions)])\n",
    "            else:\n",
    "                # Demás acciones, aplicamos negativamente\n",
    "                target_policy_s[a_i] -= delta_all[a_i]\n",
    "        # Actualizamos la red neuronal\n",
    "        target_policy_s = normalize(target_policy_s)\n",
    "        loss, grads = self.policy.backward(np.array([current_policy_s]), np.array([target_policy_s]))\n",
    "        self.policy.apply_grads(grads)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Finalmente, implementamos los métodos públicos: `learn` y `select_action`."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class WoLFPHC(WoLFPHC):\n",
    "    def learn(self, joint_action, rewards, state, next_state):\n",
    "        # Sólo consideramos lo que respecta a nuestro agente\n",
    "        agent_action = joint_action[self.agent_id]\n",
    "        agent_reward = rewards[self.agent_id]\n",
    "        one_hot_state = one_hot(state, self.game.num_states)\n",
    "        one_hot_next_state = one_hot(next_state, self.game.num_states)\n",
    "        next_value = np.max(self.q_model.forward(one_hot_next_state)[0])\n",
    "\n",
    "        # Actualizamos valor Q\n",
    "        td_error, loss = self.update_q(self.q_model, one_hot_state, agent_action, agent_reward, next_value)\n",
    "\n",
    "        # Actualizamos política media\n",
    "        self.visit_counts[state] += 1\n",
    "        self.update_average_policy(state)\n",
    "        self.update_policy(state)\n",
    "\n",
    "        # Guardamos el error de diferencia temporal y la pérdida de la red neuronal para estadísticas posteriores\n",
    "        self.metrics['td_error'].append(abs(td_error))\n",
    "        self.metrics['loss'].append(abs(loss))\n",
    "\n",
    "    def select_action(self, state, train=True):\n",
    "        if train and self.rng.random() < self.epsilon:\n",
    "                return self.rng.choice(range(self.game.num_actions))\n",
    "        else:\n",
    "            np.random.seed(self.rng.randint(0, 10000))\n",
    "            return np.random.choice(range(self.game.num_actions), p=normalize(self.q_model.forward(one_hot(state, self.game.num_states))[0]))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Experimento: Rock, Paper, Scissors"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def train_rps_2_agents(num_turns, learning_rates_loss, learning_rates_win,\n",
    "                       gammas, alphas, epsilons, seeds):\n",
    "    game_model = GameModel(num_agents=2, num_states=1, num_actions=3)\n",
    "    algorithm_player_0 = WoLFPHC(0, game_model,\n",
    "                                 learning_rate_loss=learning_rates_loss[0],\n",
    "                                 learning_rate_win=learning_rates_win[0],\n",
    "                                 gamma=gammas[0], alpha=alphas[0], epsilon=epsilons[0], seed=seeds[0])\n",
    "    algorithm_player_1 = WoLFPHC(1, game_model,\n",
    "                                 learning_rate_loss=learning_rates_loss[1],\n",
    "                                 learning_rate_win=learning_rates_win[1],\n",
    "                                 gamma=gammas[1], alpha=alphas[1], epsilon=epsilons[1], seed=seeds[1])\n",
    "    env = rps_v2.parallel_env(max_cycles=num_turns, render_mode=\"ansi\")\n",
    "\n",
    "    cumulative_rewards, actions_played = train(env, game_model, normal_form_obs_to_state,\n",
    "                                               [algorithm_player_0, algorithm_player_1])\n",
    "\n",
    "    env.close()\n",
    "    return game_model, algorithm_player_0, algorithm_player_1, cumulative_rewards, actions_played"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b1896779823404df",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "game_model, algorithm_player_0, algorithm_player_1, cumulative_rewards, actions_played = \\\n",
    "        train_rps_2_agents(num_turns=15, learning_rates_loss=[0.001, 0.001],\n",
    "                           learning_rates_win=[0.00001, 0.00001],\n",
    "                           gammas=[0.95, 0.95], alphas=[0.0001, 0.0001],\n",
    "                           epsilons=[0.2, 0.2], seeds=[0, 1])\n",
    "\n",
    "# Recompensa acumulada. Debería ser [0, 0] en el infinito, si las estrategias son óptimas\n",
    "print(f\"Recompensas acumuladas: {cumulative_rewards[-1][0]}, {cumulative_rewards[-1][1]}\")\n",
    "\n",
    "# Espacio de acciones conjuntas\n",
    "print(f\"Espacio de acciones conjuntas ordenado: {game_model.action_space}\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "84f33549b95225b6",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Valores Q calculados por los dos agentes:\n",
    "print(\"Valores Q calculados por el agente 0:\")\n",
    "print(pretty_print_array(algorithm_player_0.q_model.forward(one_hot(0, game_model.num_states))[0]))\n",
    "print(\"Valores Q calculados por el agente 1:\")\n",
    "print(pretty_print_array(algorithm_player_1.q_model.forward(one_hot(0, game_model.num_states))[0]))\n",
    "\n",
    "# Política del agente 0:\n",
    "print(f\"Política del agente 0: {normalize(algorithm_player_0.policy.forward(one_hot(0, game_model.num_states))[0])}\")\n",
    "\n",
    "# Política del agente 1:\n",
    "print(f\"Política del agente 1: {normalize(algorithm_player_1.policy.forward(one_hot(0, game_model.num_states))[0])}\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "95b0dbdbe1b4c3b0",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "draw_history(algorithm_player_0.metrics[\"td_error\"], \"TD Error\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "10f1eb7a113173c3",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "draw_history(algorithm_player_0.metrics[\"loss\"], \"Loss\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ffed1dc219dc26c3",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Experimento: dilema del prisionero"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import prisoners_dilemma"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f00aebaffbe3b999",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def train_pd(num_turns, learning_rates_loss, learning_rates_win, gammas, alphas, epsilons, seeds):\n",
    "    game_model = GameModel(num_agents=2, num_states=1, num_actions=2)\n",
    "    algorithm_player_0 = WoLFPHC(0, game_model,\n",
    "                                 learning_rate_loss=learning_rates_loss[0],\n",
    "                                 learning_rate_win=learning_rates_win[0],\n",
    "                                 gamma=gammas[0], alpha=alphas[0], epsilon=epsilons[0], seed=seeds[0])\n",
    "    algorithm_player_1 = WoLFPHC(1, game_model,\n",
    "                                 learning_rate_loss=learning_rates_loss[1],\n",
    "                                 learning_rate_win=learning_rates_win[1],\n",
    "                                 gamma=gammas[1], alpha=alphas[1], epsilon=epsilons[1], seed=seeds[1])\n",
    "    env = prisoners_dilemma.parallel_env(max_cycles=num_turns, render_mode=\"ansi\")\n",
    "    cumulative_rewards, actions_played = train(env, game_model, normal_form_obs_to_state,\n",
    "                                               [algorithm_player_0, algorithm_player_1])\n",
    "    return game_model, algorithm_player_0, algorithm_player_1, cumulative_rewards, actions_played"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "92f1827b77f9b26b",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "game_model, algorithm_player_0, algorithm_player_1, cumulative_rewards, actions_played = \\\n",
    "    train_pd(num_turns=1, learning_rates_loss=[0.001, 0.001], learning_rates_win=[0.00001, 0.00001],\n",
    "             gammas=[0.95, 0.95], alphas=[0.01, 0.01], epsilons=[0.2, 0.2], seeds=[0, 1])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9c8f6d09706afe2c",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Recompensa acumulada\n",
    "print(f\"Recompensas acumuladas: {cumulative_rewards[-1][0]}, {cumulative_rewards[-1][1]}\")\n",
    "\n",
    "# Espacio de acciones conjuntas\n",
    "print(f\"Espacio de acciones conjuntas ordenado: {game_model.action_space}\")\n",
    "\n",
    "# Valores Q calculados por los dos agentes:\n",
    "print(\"Valores Q calculados por el agente 0:\")\n",
    "print(pretty_print_array(algorithm_player_0.q_model.forward(one_hot(0, game_model.num_states))))\n",
    "print(\"Valores Q calculados por el agente 1:\")\n",
    "print(pretty_print_array(algorithm_player_1.q_model.forward(one_hot(0, game_model.num_states))))\n",
    "\n",
    "# Política del agente 0:\n",
    "print(f\"Política del agente 0: {normalize(algorithm_player_0.policy.forward(one_hot(0, game_model.num_states))[0])}\")\n",
    "\n",
    "# Política del agente 1:\n",
    "print(f\"Política del agente 1: {normalize(algorithm_player_1.policy.forward(one_hot(0, game_model.num_states))[0])}\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4075bcb087598037",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "draw_history(algorithm_player_0.metrics[\"td_error\"], \"TD Error\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4265f6f0317b1588",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "draw_history(algorithm_player_0.metrics[\"loss\"], \"Loss\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "80bc0a28c7f3472",
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
