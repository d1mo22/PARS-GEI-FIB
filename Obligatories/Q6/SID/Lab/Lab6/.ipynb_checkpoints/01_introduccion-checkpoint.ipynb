{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Lab: Aprendizaje por refuerzo (I)\n",
    "# 1 - Gymnasium y entorno FrozenLake-v2"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6c81d9ce31ea3596"
  },
  {
   "cell_type": "markdown",
   "source": [
    "En esta sesión vamos a ver Gymnasium, una librería de referencia para probar algoritmos de aprendizaje por refuerzo que incluye algunos entornos y además define una interfaz común para integrar entornos nuevos basada en el bucle del agente inteligente que ya hemos visto en clase de teoría."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e26f11c45d63e7e7"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Configuración y dependencias"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "145886c0d320e54d"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Declaración de constantes\n",
    "SLIPPERY = False\n",
    "NUM_EPISODES = 50"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "effb511a40e72b79",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "!pip install gymnasium seaborn numpy"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "63b71fa5939d6f17",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6aa0e018a545e861",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Entorno FrozenLake-v1\n",
    "\n",
    "Vamos a hacer pruebas con el entorno FrozenLake-v1. En el enlace [https://gymnasium.farama.org/environments/toy_text/frozen_lake/](https://gymnasium.farama.org/environments/toy_text/frozen_lake/) tenéis una descripción detallada del entorno. Dedicadle unos minutos a leerlo con calma para poder entender el efecto de los diferentes algoritmos sobre el proceso de aprendizaje.\n",
    "\n",
    "Lo primero que vamos a hacer es ejecutar el entorno con una política totalmente aleatoria. Cuando ejecutéis la siguiente celda aparecerá una ventana con el entorno. Puede que se oculte detrás de las demás ventanas por lo que quizá tendréis que buscarla. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9752d95c1d7ea7af"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "env = gym.make(\"FrozenLake-v1\", desc=None, map_name=\"4x4\", render_mode=\"human\", is_slippery=False) \n",
    "env.reset()\n",
    "env.render()\n",
    "is_done = False\n",
    "t = 0\n",
    "\n",
    "while not is_done:\n",
    "   action = env.action_space.sample()\n",
    "   state, reward, is_done, truncated, _ = env.step(action)\n",
    "   env.render()\n",
    "   t +=1\n",
    "        \n",
    "print(\"\\nlast state  =\", state)\n",
    "print(\"reward =     \", reward)\n",
    "print(\"terminated = \", is_done)\n",
    "print(\"truncated  = \", truncated)\n",
    "print(\"time steps = \", t)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ab71258821f20df4",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Como vemos, con la función `step()` del entorno ejecutamos una acción. En este caso, con la llamada a `env.action_space.sample()` estamos obteniendo una acción aleatoria del espacio de acciones (el elemento `A` de la tupla del MDP). Como resultado de ejecutar la acción, recibimos:\n",
    "\n",
    "* `state`: el nuevo estado en el que está el agente.\n",
    "* `reward`: la recompensa obtenida al haber llegado al nuevo estado desde el anterior mediante la acción ejecutada.\n",
    "* `terminated`: un booleano que representa si hemos llegado a un estado final.\n",
    "* `truncated`: un booleano que nos dice si el entorno se ha detenido de manera inesperada (e.g. por un hipotético límite de tiempo).\n",
    "* `info`: una estructura con metadatos y monitorización, que no usaremos en esta sesión.\n",
    "\n",
    "Vamos ahora a encapsular esta política en una clase que representará nuestro agente y que iremos evolucionando y modificando:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3359f29eaceeb1e0"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class RandomAgent:\n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "\n",
    "    def select_action(self):\n",
    "        return self.env.action_space.sample()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d8a1a79b2babf4db",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Vamos a definir también el entorno que vamos a usar en el resto de ejemplos:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ee4f44a73fb38cf0"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "env = gym.make(\"FrozenLake-v1\", desc=None, map_name=\"4x4\", render_mode=\"human\", is_slippery=SLIPPERY)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1fdd7c1b1e59404c",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Replicamos ahora el resultado de antes, pero ahora con la clase RandomAgent:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8d034aceedeefd69"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "env.reset()\n",
    "env.render()\n",
    "is_done = False\n",
    "t = 0\n",
    "agent = RandomAgent(env)\n",
    "while not is_done:\n",
    "   action = agent.select_action()\n",
    "   state, reward, is_done, truncated, _ = env.step(action)\n",
    "   env.render()\n",
    "   t += 1\n",
    "        \n",
    "print(\"\\nlast state  =\", state)\n",
    "print(\"reward =     \", reward)\n",
    "print(\"terminated = \", is_done)\n",
    "print(\"truncated  = \", truncated)\n",
    "print(\"time steps = \", t)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7375043607cf1676",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Obviamente parece que el agente no va a resolver esta tarea (a menos que hayáis tenido muchísima suerte). Vamos a realizar unas cuantas ejecuciones para sacar resultados empíricos:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "62551a75c6bdcd26"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def test_episode(agent, env):\n",
    "    env.reset()\n",
    "    is_done = False\n",
    "\n",
    "    while not is_done:\n",
    "        action = agent.select_action()\n",
    "        state, reward, is_done, truncated, info = env.step(action)\n",
    "    return state, reward, is_done, truncated, info\n",
    "\n",
    "agent = RandomAgent(env)\n",
    "\n",
    "solved = 0\n",
    "rewards = []\n",
    "for episode in range(50):\n",
    "    state, reward, is_done, truncated, _ = test_episode(agent, env)\n",
    "    rewards.append(reward)\n",
    "    if state == 15:\n",
    "        solved += 1\n",
    "\n",
    "print(\"Solved\", solved, \"times (\", solved * 2, \"%)\")  "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1782dacd1b2472a1",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "data = pd.DataFrame({'Episode': range(1, len(rewards) + 1), 'Reward': rewards})\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.lineplot(x='Episode', y='Reward', data=data)\n",
    "\n",
    "plt.title('Rewards Over Episodes')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Reward')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9ba9912f3e37304f",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "En el siguiente notebook vamos a comenzar a aplicar algoritmos más fiables."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f220498382490180"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "bf5bd220032c1ba1"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
