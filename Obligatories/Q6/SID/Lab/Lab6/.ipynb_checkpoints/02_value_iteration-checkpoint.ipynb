{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Lab: Aprendizaje por refuerzo (I)\n",
    "# 2 - Iteración de valor"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f352ceef0d605988"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Configuración y dependencias"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1a24f9cf8d162a2c"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Declaración de constantes\n",
    "SLIPPERY = False\n",
    "T_MAX = 15\n",
    "NUM_EPISODES = 5\n",
    "GAMMA = 0.95\n",
    "REWARD_THRESHOLD = 0.9"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3c4663d4e7a202af",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "!pip install gymnasium seaborn numpy"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7ca4893610eda720",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "70c6aebaf6bcdd3",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "env = gym.make(\"FrozenLake-v1\", desc=None, map_name=\"4x4\", render_mode=\"human\", is_slippery=SLIPPERY)\n",
    "\n",
    "def test_episode(agent, env):\n",
    "    env.reset()\n",
    "    is_done = False\n",
    "    t = 0\n",
    "\n",
    "    while not is_done:\n",
    "        action = agent.select_action()\n",
    "        state, reward, is_done, truncated, info = env.step(action)\n",
    "        t += 1\n",
    "    return state, reward, is_done, truncated, info\n",
    "\n",
    "def draw_rewards(rewards):\n",
    "    data = pd.DataFrame({'Episode': range(1, len(rewards) + 1), 'Reward': rewards})\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.lineplot(x='Episode', y='Reward', data=data)\n",
    "\n",
    "    plt.title('Rewards Over Episodes')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Reward')\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5ae63477f82d8c2b",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Tarea de entorno como MDP"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "788030c15c551bd2"
  },
  {
   "cell_type": "markdown",
   "source": [
    "En algunos entornos de Gymnasium, especialmente en los basados en espacios discretos de estados y acciones, es posible acceder a las funciones de transición y recompensa:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ba31dde32bdb1a17"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "env.unwrapped.P"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1e9472f9e04280bb",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "En esta estructura podéis encontrar un diccionario de estados, donde para cada estado hay un diccionario de acciones, definiendo así pares de estado-acción. Para cada uno de estos pares, hay una lista de tuplas representando los posibles estados a los que se puede llegar y en qué condiciones. Cada tupla contiene, por este orden: la probabilidad de llegar a partir del estado y la acción correspondientes a las claves de los diccionarios, el estado al que se llega, la recompensa y un booleano indicando si se ha llegado a un estado final.\n",
    "\n",
    "Debido a que el estado 15 es el único que recibe recompensa, desde el estado 14 mediante la acción 2, podemos deducir que 15 es el estado que está en la esquina inferior izquierda y que la acción 2 se corresponde con la acción \"Right\". **Completa el siguiente código para obtener un agente que completa el entorno con una política prediseñada:**"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2ce66bf74fe949ec"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class PlanAgent:\n",
    "    def __init__(self):\n",
    "        self.actions = {'Right': 2}\n",
    "        self.good_plan = 2 * ['Right'] + ['Right'] + ['Right'] + 2 * ['Right']\n",
    "        self.step = 0\n",
    "\n",
    "    def select_action(self):\n",
    "        action = self.good_plan[self.step]\n",
    "        self.step = (self.step + 1) % len(self.good_plan)\n",
    "        return self.actions[action]\n",
    "\n",
    "    def reset(self):\n",
    "        self.step = 0"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a0a67b0ef58c26bb",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "agent = PlanAgent()\n",
    "is_done = False\n",
    "rewards = []\n",
    "for n_ep in range(NUM_EPISODES):\n",
    "    print('Episode: ', n_ep)\n",
    "    env.reset()\n",
    "    total_reward = 0\n",
    "    for i in range(T_MAX):\n",
    "        action = agent.select_action()\n",
    "        state, reward, is_done, truncated, _ = env.step(action)\n",
    "        total_reward = total_reward + reward\n",
    "        env.render()\n",
    "        if is_done:\n",
    "            break\n",
    "    rewards.append(total_reward)\n",
    "draw_rewards(rewards)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4d66959411621fac",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Aprovechando el MDP: algoritmo de *value iteration*"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d2e4d22baf388c5f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Aquí tenéis una posible implementación del algoritmo de iteración de valor para la ecuación V*, usando arrays de NumPy. Observad cómo se actualizan los valores de V y cómo se utiliza el MDP que se puede consultar en el objeto del entorno:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "17afd7b3325b67b9"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class ValueIterationAgent:\n",
    "    def __init__(self, env, gamma):\n",
    "        self.env = env\n",
    "        self.V = np.zeros(self.env.observation_space.n)\n",
    "        self.gamma = gamma\n",
    "        \n",
    "    def calc_action_value(self, state, action):\n",
    "        action_value = sum([prob * (reward + self.gamma * self.V[next_state])\n",
    "                            for prob, next_state, reward, _ \n",
    "                            in self.env.unwrapped.P[state][action]]) \n",
    "        return action_value\n",
    "\n",
    "    def select_action(self, state):\n",
    "        best_action = best_value = None\n",
    "        for action in range(self.env.action_space.n):\n",
    "            action_value = self.calc_action_value(state, action)\n",
    "            if not best_value or best_value < action_value:\n",
    "                best_value = action_value\n",
    "                best_action = action\n",
    "        return best_action\n",
    "\n",
    "    def value_iteration(self):\n",
    "        max_diff = 0\n",
    "        for state in range(self.env.observation_space.n):\n",
    "            state_values = []\n",
    "            for action in range(self.env.action_space.n):  \n",
    "                state_values.append(self.calc_action_value(state, action))\n",
    "            new_V = max(state_values)\n",
    "            diff = abs(new_V - self.V[state])\n",
    "            if diff > max_diff:\n",
    "                max_diff = diff\n",
    "            self.V[state] = new_V\n",
    "        return self.V, max_diff\n",
    "    \n",
    "    def policy(self):   \n",
    "        policy = np.zeros(env.observation_space.n) \n",
    "        for s in range(env.observation_space.n):\n",
    "            Q_values = [self.calc_action_value(s,a) for a in range(self.env.action_space.n)] \n",
    "            policy[s] = np.argmax(np.array(Q_values))        \n",
    "        return policy"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d630b18cbd8592fb",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Definimos una función para decidir cuándo parar la iteración de valor. También definimos una función para automatizar el entrenamiento. El algoritmo de iteración de valor se considera un algoritmo de aprendizaje por refuerzo activo y basado en modelo (que depende de un MDP).\n",
    "\n",
    "En clase de teoría hemos visto la convergencia basada en la máxima diferencia entre valores de estados. Otra opción es la de definir un umbral de recompensa a partir del cual parar:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f48f216b4b45b527"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def check_improvements():\n",
    "    reward_test = 0.0\n",
    "    for i in range(NUM_EPISODES):\n",
    "        total_reward = 0.0\n",
    "        state, _ = env.reset()\n",
    "        for i in range(T_MAX):\n",
    "            action = agent.select_action(state)\n",
    "            new_state, new_reward, is_done, truncated, _ = env.step(action)\n",
    "            total_reward += new_reward\n",
    "            if is_done: \n",
    "                break\n",
    "            state = new_state\n",
    "        reward_test += total_reward\n",
    "    reward_avg = reward_test / NUM_EPISODES\n",
    "    return reward_avg\n",
    "\n",
    "def train(agent): \n",
    "    rewards = []\n",
    "    max_diffs = []\n",
    "    t = 0\n",
    "    best_reward = 0.0\n",
    "     \n",
    "    while best_reward < REWARD_THRESHOLD:\n",
    "        _, max_diff = agent.value_iteration()\n",
    "        max_diffs.append(max_diff)\n",
    "        print(\"After value iteration, max_diff = \" + str(max_diff))\n",
    "        t += 1\n",
    "        reward_test = check_improvements()\n",
    "        rewards.append(reward_test)\n",
    "               \n",
    "        if reward_test > best_reward:\n",
    "            print(f\"Best reward updated {reward_test:.2f} at iteration {t}\") \n",
    "            best_reward = reward_test\n",
    "    \n",
    "    return rewards, max_diffs"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "531f1256eb8eab79",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Ahora ya podemos lanzar un entrenamiento:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b7a7f72db1324eeb"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "agent = ValueIterationAgent(env, gamma=GAMMA)\n",
    "rewards, max_diffs = train(agent)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "246f7378e3e5ea23",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "**¿Qué es lo que ocurre durante los primeros episodios? ¿Cómo lo interpretáis, por lo que conocéis del algoritmo?**\n",
    "\n",
    "**Probad a modificar el código para que la parada sea por la diferencia entre valores de estado, poniendo un umbral que os parezca aceptable. ¿Qué método de parada os parece mejor en este escenario?**"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c23b36ee2611bd23"
  },
  {
   "cell_type": "markdown",
   "source": [
    "La clase del agente incluye un método para calcular la política:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9cc2b9c440e9fcac"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def print_policy(policy):\n",
    "    visual_help = {0:'<', 1:'v', 2:'>', 3:'^'}\n",
    "    policy_arrows = [visual_help[x] for x in policy]\n",
    "    print(np.array(policy_arrows).reshape([-1, 4]))\n",
    "    \n",
    "policy = agent.policy()\n",
    "print_policy(policy)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cafe9fe4f31b6734",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Una vez que el agente está entrenado, lo podemos poner en el entorno para probar su rendimiento:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "322261a5a87dad8a"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "is_done = False\n",
    "rewards = []\n",
    "for n_ep in range(NUM_EPISODES):\n",
    "    state, _ = env.reset()\n",
    "    print('Episode: ', n_ep)\n",
    "    total_reward = 0\n",
    "    for i in range(T_MAX):\n",
    "        action = agent.select_action(state)\n",
    "        state, reward, is_done, truncated, _ = env.step(action)\n",
    "        total_reward = total_reward + reward\n",
    "        env.render()\n",
    "        if is_done:\n",
    "            break\n",
    "    rewards.append(total_reward)\n",
    "draw_rewards(rewards)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f1a9eeb6d114a944",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "4611848dbfe2fce",
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
