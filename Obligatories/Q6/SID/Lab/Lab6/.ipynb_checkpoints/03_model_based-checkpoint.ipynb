{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Lab: Aprendizaje por refuerzo (I)\n",
    "# 3 - Model-based"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dadca7b535b81fa9"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Configuración y dependencias"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cac12d201206df61"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Declaración de constantes\n",
    "SLIPPERY = False\n",
    "T_MAX = 15\n",
    "NUM_EPISODES = 5\n",
    "GAMMA = 0.95\n",
    "REWARD_THRESHOLD = 0.9"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ccc2e424d97f4937",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "!pip install gymnasium seaborn numpy"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ba52fdcd59164ab7",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import collections"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "748ab422de7d4566",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "env = gym.make(\"FrozenLake-v1\", desc=None, map_name=\"4x4\", render_mode=\"human\", is_slippery=SLIPPERY)\n",
    "\n",
    "def test_episode(agent, env):\n",
    "    env.reset()\n",
    "    is_done = False\n",
    "    t = 0\n",
    "\n",
    "    while not is_done:\n",
    "        action = agent.select_action()\n",
    "        state, reward, is_done, truncated, info = env.step(action)\n",
    "        t += 1\n",
    "    return state, reward, is_done, truncated, info\n",
    "\n",
    "def draw_rewards(rewards):\n",
    "    data = pd.DataFrame({'Episode': range(1, len(rewards) + 1), 'Reward': rewards})\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.lineplot(x='Episode', y='Reward', data=data)\n",
    "\n",
    "    plt.title('Rewards Over Episodes')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Reward')\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "def check_improvements():\n",
    "    reward_test = 0.0\n",
    "    for i in range(NUM_EPISODES):\n",
    "        total_reward = 0.0\n",
    "        state, _ = env.reset()\n",
    "        for i in range(T_MAX):\n",
    "            action = agent.select_action(state)\n",
    "            new_state, new_reward, is_done, truncated, _ = env.step(action)\n",
    "            total_reward += new_reward\n",
    "            if is_done: \n",
    "                break\n",
    "            state = new_state\n",
    "        reward_test += total_reward\n",
    "    reward_avg = reward_test / NUM_EPISODES\n",
    "    return reward_avg\n",
    "\n",
    "def train(agent): \n",
    "    rewards = []\n",
    "    max_diffs = []\n",
    "    t = 0\n",
    "    best_reward = 0.0\n",
    "     \n",
    "    while best_reward < REWARD_THRESHOLD:\n",
    "        _, max_diff = agent.value_iteration()\n",
    "        max_diffs.append(max_diff)\n",
    "        print(\"After value iteration, max_diff = \" + str(max_diff))\n",
    "        t += 1\n",
    "        reward_test = check_improvements()\n",
    "        rewards.append(reward_test)\n",
    "               \n",
    "        if reward_test > best_reward:\n",
    "            print(f\"Best reward updated {reward_test:.2f} at iteration {t}\") \n",
    "            best_reward = reward_test\n",
    "    \n",
    "    return rewards, max_diffs\n",
    "\n",
    "def print_policy(policy):\n",
    "    visual_help = {0:'<', 1:'v', 2:'>', 3:'^'}\n",
    "    policy_arrows = [visual_help[x] for x in policy]\n",
    "    print(np.array(policy_arrows).reshape([-1, 4]))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "579331caa2ca91d7",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Estimación directa: un método *model-based*"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d673192127a6a0de"
  },
  {
   "cell_type": "markdown",
   "source": [
    "En clase de teoría hemos visto Monte Carlo como método basado en modelo, pero todavía no hemos visto el algoritmo de búsqueda. Vamos a ver otro método basado en modelo que es más sencillo (y también, en general, menos eficiente): el método de estimación directa:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ecf36227d1a24719"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class DirectEstimationAgent:\n",
    "    def __init__(self, env, gamma, num_trajectories):\n",
    "        self.env = env\n",
    "        self.state, _ = self.env.reset()\n",
    "        self.rewards = collections.defaultdict(float)\n",
    "        self.transits = collections.defaultdict(collections.Counter)\n",
    "        self.V = np.zeros(self.env.observation_space.n)\n",
    "        self.gamma = gamma\n",
    "        self.num_trajectories = num_trajectories\n",
    "\n",
    "    def play_n_random_steps(self, count):\n",
    "        for _ in range(count):\n",
    "            action = self.env.action_space.sample()\n",
    "            new_state, reward, is_done, truncated, _ = self.env.step(action)\n",
    "            self.rewards[(self.state, action, new_state)] = reward\n",
    "            self.transits[(self.state, action)][new_state] += 1\n",
    "            if is_done:\n",
    "                self.state, _ = self.env.reset() \n",
    "            else: \n",
    "                self.state = new_state\n",
    "\n",
    "    def calc_action_value(self, state, action):\n",
    "        target_counts = self.transits[(state, action)]\n",
    "        total = sum(target_counts.values())\n",
    "        action_value = 0.0\n",
    "        for s_, count in target_counts.items():\n",
    "            r = self.rewards[(state, action, s_)]\n",
    "            prob = (count / total)\n",
    "            action_value += prob*(r + self.gamma * self.V[s_])\n",
    "        return action_value\n",
    "\n",
    "    def select_action(self, state):\n",
    "        best_action, best_value = None, None\n",
    "        for action in range(self.env.action_space.n):\n",
    "            action_value = self.calc_action_value(state, action)\n",
    "            if best_value is None or best_value < action_value:\n",
    "                best_value = action_value\n",
    "                best_action = action\n",
    "        return best_action\n",
    "\n",
    "    def value_iteration(self):\n",
    "        self.play_n_random_steps(self.num_trajectories)\n",
    "        max_diff = 0\n",
    "        for state in range(self.env.observation_space.n):\n",
    "            state_values = [\n",
    "                self.calc_action_value(state, action)\n",
    "                for action in range(self.env.action_space.n)\n",
    "            ]\n",
    "            new_V = max(state_values)\n",
    "            diff = abs(new_V - self.V[state])\n",
    "            if diff > max_diff:\n",
    "                max_diff = diff\n",
    "            self.V[state] = new_V\n",
    "        return self.V, max_diff\n",
    "    \n",
    "    def policy(self):   \n",
    "        policy = np.zeros(env.observation_space.n) \n",
    "        for s in range(env.observation_space.n):\n",
    "            Q_values = [self.calc_action_value(s,a) for a in range(self.env.action_space.n)] \n",
    "            policy[s] = np.argmax(np.array(Q_values))        \n",
    "        return policy"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "639ba22b772d9000",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Este método es similar al que hemos visto de Monte Carlo pero sin un proceso sofisticado que nos guíe en qué trayectorias generar. En estimación directa, se simula en cada iteración un número de trayectorias aleatorias, y a partir de ellas, calculando probabilidades a partir de frecuencias para aproximar las funciones de transición y recompensa y posteriormente se calcula el valor de los estados usando iteración de valor.\n",
    "\n",
    "Vamos a probarlo:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "17ea51137bdff1a5"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "agent = DirectEstimationAgent(env, gamma=GAMMA, num_trajectories=10)\n",
    "train(agent)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2a1fd7e4508ddc12",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Jugad con el parámetro de número de trayectorias por iteración.**\n",
    "\n",
    "**¿Cuántas iteraciones habéis tenido que esperar (si es que vuestro agente lo consigue, claro)? ¿Qué problema veis en este método para esta tarea en particular?**\n",
    "\n",
    "Una vez el agente está entrenado, podemos inspeccionar su política y ponerlo a prueba como hicimos con el ValueIterationAgent:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5c85fe9a467e3922"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "is_done = False\n",
    "rewards = []\n",
    "for n_ep in range(NUM_EPISODES):\n",
    "    state, _ = env.reset()\n",
    "    print('Episode: ', n_ep)\n",
    "    total_reward = 0\n",
    "    for i in range(T_MAX):\n",
    "        action = agent.select_action(state)\n",
    "        state, reward, is_done, truncated, _ = env.step(action)\n",
    "        total_reward = total_reward + reward\n",
    "        env.render()\n",
    "        if is_done:\n",
    "            break\n",
    "    rewards.append(total_reward)\n",
    "draw_rewards(rewards)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8d7c2739bdc4916"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "4b74c3b88a2c8b0d"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
