{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DWQt9bKuzyOT"
   },
   "source": [
    "# CAIM Lab Session 2: Intro to ElasticSearch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gMxLVHngzyOV"
   },
   "source": [
    "In this session you will learn:\n",
    "\n",
    "- a few basics on the `ElasticSearch` database\n",
    "- how to index a set of documents and how to ask simple queries about these documents\n",
    "- how to do this from `Python`\n",
    "- based on the previous, you will compute the boolean and tf-idf matrix for the toy corpus used in class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7imRMz0wzyOW"
   },
   "source": [
    "## 1. ElasticSearch\n",
    "\n",
    "[ElasticSearch](https://www.elastic.co/) is a _NoSQL/document_ database with the capability of indexing and searching text documents. As a rough analogue, we can use the following table for the equivalence between ElasticSearch and a more classical relational database:\n",
    "\n",
    "| Relational DB | ElasticSearch |\n",
    "|---|---|\n",
    "| Database | Index |\n",
    "| Row / record | Document |\n",
    "| Column | Field |\n",
    "\n",
    "An index can be thought of as an optimized collection of documents and each document is a collection of fields, which are the key-value pairs that contain your data.\n",
    "\n",
    "`ElasticSearch` is a pretty big beast with many options. Luckily, there is much documentation, a few useful links are:\n",
    "\n",
    "- Here is the [full documentation](https://www.elastic.co/guide/en/elasticsearch/reference/current/index.html)\n",
    "- Intros you may want to have a look at:\n",
    "    - https://medium.com/expedia-group-tech/getting-started-with-elastic-search-6af62d7df8dd\n",
    "    - http://joelabrahamsson.com/elasticsearch-101\n",
    "- You found another one that you liked? Let us know."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HDLPESJXzyOW"
   },
   "source": [
    "## 2. Running ElasticSearch\n",
    "\n",
    "This database runs as a web service in a machine and can be accessed using a REST\n",
    "web API.\n",
    "\n",
    "The ElasticSearch binaries are in `/opt/elasticsearch-8.2.2/`.\n",
    "\n",
    "Depending on the disk space that you have available you can run directly the script that starts the\n",
    "database, so all the data will be stored in your user directory, or you can change the configuration\n",
    "of the database in order to use the space in `/tmp`. \n",
    "Also, security needs to be disabled (through the `xpack.security.enabled` configuration option) so make\n",
    "sure the line is found in the configuration file.\n",
    "\n",
    "```bash\n",
    "cp -r /opt/elasticsearch-8.2.2/config/ /tmp\n",
    "```\n",
    "\n",
    "Modify or add the following lines to `/tmp/config/elasticsearch.yml`:\n",
    "\n",
    "```\n",
    "path.data : /tmp/elastic_data\n",
    "path.logs : /tmp/elastic_logs\n",
    "xpack.security.enabled : false\n",
    "xpack.security.enrollment.enabled : false\n",
    "```\n",
    "\n",
    "Set the environment variable `ES_PATH_CONF` to point to the configuration files. Example (if using `tcsh`):\n",
    "\n",
    "```bash\n",
    "setenv ES_PATH_CONF /tmp/config\n",
    "```\n",
    "\n",
    "Now you can run ElasticSearch with:\n",
    "\n",
    "```bash\n",
    "/opt/elasticsearch-8.2.2/bin/elasticsearch\n",
    "```\n",
    "\n",
    "After a few seconds (and a lot of logging) the database will be up and running; you may need to hit return for the prompt to show up. To test whether `ElasticSearch` is working execute the code in the cell below. __The database needs to be running throughout the execution of this script, otherwise you will get a connection error.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "XpULY78hzyOY",
    "outputId": "357be68e-a6ed-4b44-880b-6311ab70054a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(b'{\\n  \"name\" : \"10-192-204-88client.eduroam.upc.edu\",\\n  \"cluster_name\" : \"'\n",
      " b'elasticsearch\",\\n  \"cluster_uuid\" : \"nhigYoLMQYqHsoEar5DFlw\",\\n  \"version\"'\n",
      " b' : {\\n    \"number\" : \"9.1.4\",\\n    \"build_flavor\" : \"default\",\\n    \"build_'\n",
      " b'type\" : \"tar\",\\n    \"build_hash\" : \"0b7fe68d2e369469ff9e9f344ab6df64ab9c5'\n",
      " b'293\",\\n    \"build_date\" : \"2025-09-16T22:05:19.073893347Z\",\\n    \"build_sn'\n",
      " b'apshot\" : false,\\n    \"lucene_version\" : \"10.2.2\",\\n    \"minimum_wire_comp'\n",
      " b'atibility_version\" : \"8.19.0\",\\n    \"minimum_index_compatibility_version\"'\n",
      " b' : \"8.0.0\"\\n  },\\n  \"tagline\" : \"You Know, for Search\"\\n}\\n')\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "import requests\n",
    "\n",
    "try:\n",
    "    resp = requests.get('http://localhost:9200/')\n",
    "    pprint(resp.content)\n",
    "\n",
    "except Exception:\n",
    "    print('elasticsearch is not running')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PPzcxW4EzyOZ"
   },
   "source": [
    "If `ElasticSearch` is working you will see an answer from the server; otherwise you will see a message indicating that it is not running. You can try also throwing the URL http://localhost:9200 to your browser; you should get a similar answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vYwBEaF9zyOa"
   },
   "source": [
    "## 3. Indexing and querying\n",
    "\n",
    "`ElasticSearch` is a database that allows storing documents (tables do not need a predefined schema as in relational databases). Text in these documents can be processed so the queries extend beyond exact matches allowing complex queries, fuzzy matching and ranking documents respect to the actual match.\n",
    "\n",
    "These kinds of databases are behind search engines like Google Search or Bing.\n",
    "\n",
    "There are different ways of operating with ElasticSearch. It is deployed esentially as a web service with a REST API, so it can be accessed basically from any language with a library for operating with HTTP servers.\n",
    "\n",
    "We are going to use two python libraries for programming on top of ElasticSearch: `elasticsearch` and `elasticsearch-dsl`. Both provide access to ElasticSearch functionalities hiding and making more programming-friendly the interactions, the second one is more convenient for configurating and searching. Make sure both python libraries are installed to proceed with this session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p6FyK-rAzyOa",
    "outputId": "ae7715a6-d881-4cf5-ef5e-1e3115250a41"
   },
   "outputs": [],
   "source": [
    "!pip3 install elasticsearch --user\n",
    "!pip3 install elasticsearch-dsl --user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hgJSqwPJzyOa"
   },
   "source": [
    "We are only going to see the essential elements for developing the session but feel free to learn more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IYbFciOuzyOa"
   },
   "source": [
    "To interact with ElasticSearch with need a client object of type `Elasticsearch`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "nnMxBWblzyOb"
   },
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "\n",
    "client = Elasticsearch(\"http://localhost:9200\", request_timeout=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hW1iJwWRzyOb"
   },
   "source": [
    "With this client you have a connection for operating with Elastic search. Now we will create an index. There are index operations in each library, but the one in `elasticseach-dsl` is simpler to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ARRz6OigzyOb"
   },
   "outputs": [],
   "source": [
    "from elasticsearch_dsl import Index\n",
    "\n",
    "index = Index('test', using=client)  # if it does not exist, it is created; if it does exist, then it connects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lGQiXaBizyOb"
   },
   "source": [
    "First we will need some text to index, for testing purposes we are going to use the python library `loremipsum`. We will need to install it first if it is not installed already, uncomment the code in next cell if you need to install the library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XFi4DRwLzyOb",
    "outputId": "7730895e-bf52-406b-fbe9-24fdaf75a763"
   },
   "outputs": [],
   "source": [
    "!pip3 install lorem --user  # Restart the kernel if you are not able to import the library in the next cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pva5JuVNzyOc"
   },
   "source": [
    "Now we create some random paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oPOAdx_BzyOc",
    "outputId": "37671d3e-52b6-4631-d351-80903e6dd485"
   },
   "outputs": [],
   "source": [
    "import lorem\n",
    "\n",
    "texts = [lorem.paragraph() for _ in range(10)]\n",
    "print(len(texts))\n",
    "print(texts[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aFDlDtBGzyOc"
   },
   "source": [
    "Now we can index the paragraphs in ElasticSearch using the `index` method. The document is passed as a python dictionary with the `document` parameter. The keys of the dictionary will be the fields of the document, in this case we well have only one (`text`) -- here, we use this tag but could use anything we wanted to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "45Ik8JbgzyOc",
    "outputId": "c0875521-0ec9-4d26-dc40-11b2d701d208"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing new text: Est aliquam amet quiquia quaerat consectetur porro. Quiquia quisquam n ...\n",
      "Indexing new text: Consectetur eius porro quisquam sit. Numquam modi neque etincidunt por ...\n",
      "Indexing new text: Aliquam neque tempora dolore consectetur adipisci. Modi tempora non ne ...\n",
      "Indexing new text: Amet ut modi dolorem amet etincidunt voluptatem. Sed etincidunt est qu ...\n",
      "Indexing new text: Modi porro eius dolorem aliquam sit porro quaerat. Sed ut labore modi  ...\n",
      "Indexing new text: Numquam non velit adipisci quisquam aliquam ipsum. Magnam dolore ipsum ...\n",
      "Indexing new text: Modi sed ut porro eius magnam. Neque ipsum consectetur etincidunt amet ...\n",
      "Indexing new text: Dolor quisquam est velit consectetur consectetur sit. Aliquam dolor la ...\n",
      "Indexing new text: Est velit dolore sed adipisci magnam eius. Amet non amet etincidunt ve ...\n",
      "Indexing new text: Adipisci consectetur est eius sit aliquam. Dolore neque dolore quisqua ...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ObjectApiResponse({'_shards': {'total': 2, 'successful': 1, 'failed': 0}})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for t in texts:\n",
    "    client.index(index='test', document={'text': t})\n",
    "    print(f'Indexing new text: {t[:70]} ...')\n",
    "client.indices.refresh(index='test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ostA2iSwzyOc"
   },
   "source": [
    "In case we want to get all docs in the index, we can do the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LtJ8MrYYzyOd",
    "outputId": "aeb28278-6c11-4018-fbbb-43a168b5997b"
   },
   "outputs": [],
   "source": [
    "# get all docs in index 'test'\n",
    "resp = client.search(index=\"test\", query={\"match_all\": {}})\n",
    "\n",
    "# print them\n",
    "print(f\"Got {resp['hits']['total']['value']} hits:\")\n",
    "for hit in resp['hits']['hits']:\n",
    "    pprint(hit[\"_source\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qB5KVkmLzyOd"
   },
   "source": [
    "We can also search for documents that contain a given keyword:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GVNkBalqzyOd",
    "outputId": "e7d93fde-c2ac-4207-cf12-d05b5e42a595"
   },
   "outputs": [],
   "source": [
    "from elasticsearch_dsl import Search\n",
    "\n",
    "# the following search query specifies the field where we want to search\n",
    "s_obj = Search(using=client, index='test')\n",
    "sq = s_obj.query('match', text='non')\n",
    "resp = sq.execute()\n",
    "\n",
    "print(f'Found {len(resp)} matches.')\n",
    "\n",
    "for hit in resp:\n",
    "    print(f'\\nID: {hit.meta.id}\\nText: {hit.text}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gBZSqNZ4zyOd"
   },
   "source": [
    "## 4. Counting words and docs\n",
    "\n",
    "`Elastic search` helps us to obtain the counts of words in each document. For example, the following code obtains the counts of words of a whole index by adding the counts of words obtained from each document through the functionality of `termvectors`. This function also allows us to get _document counts_ for computing tf-idf weights, by setting the `term_statistics` option to `True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Mjmw3ee_zyOd"
   },
   "outputs": [],
   "source": [
    "from elasticsearch.helpers import scan\n",
    "from collections import Counter\n",
    "\n",
    "# Search for all the documents and query the list of (word, frequency) of each one\n",
    "# Totals are accumulated using a Counter for term frequencies\n",
    "word_counts = Counter()\n",
    "sc = scan(client, index='test', query={\"query\" : {\"match_all\": {}}})\n",
    "for s in sc:\n",
    "    tv = client.termvectors(index='test', id=s['_id'], fields=['text'], term_statistics=True, positions=False)\n",
    "    if 'text' in tv['term_vectors']:   # just in case some document has no field named 'text'\n",
    "        for t in tv['term_vectors']['text']['terms']:\n",
    "            word = t\n",
    "            count = tv['term_vectors']['text']['terms'][t]['term_freq']\n",
    "            word_counts.update({word: count})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "20VwCDffzyOe",
    "outputId": "ae5da30d-0855-464e-b310-6b02c27f1beb"
   },
   "outputs": [],
   "source": [
    "# show word frequencies\n",
    "word_counts.most_common()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q521qZPPzyOe"
   },
   "source": [
    "## 5. Proposed simple exercise\n",
    "\n",
    "To get more familiar with elasticsearch, we propose that you _generate the Boolean and tf-idf matrices_ for the toy example that we used in class. You will find 7 text documents that contain the toy documents with the materials for this session in the racó. The steps to follow are:\n",
    "\n",
    "- create an empty index\n",
    "- open each text document in the `toy-docs` folder provided, read its contents and add it to the index as a new document; your index should contain 7 documents after this\n",
    "- use the `termvectors` function to obtain term counts, generate Boolean and tf-idf matrices based on these counts.\n",
    "- double check that your results coincide with the numbers in theory slides\n",
    "\n",
    "For this toy corpus, you may build dense Boolean and TF-IDF matrices (e.g., using NumPy arrays).  Note, however, that for real datasets it would be necessary to store them as sparse matrices (e.g. in compressed row store format), which would use far less memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 7 is out of bounds for axis 0 with size 7",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[57], line 45\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m vocab:\n\u001b[1;32m     44\u001b[0m                 word_idx \u001b[38;5;241m=\u001b[39m vocab\u001b[38;5;241m.\u001b[39mindex(t) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m---> 45\u001b[0m                 \u001b[43mbooleanMatrix\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdoc_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mword_idx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     46\u001b[0m booleanMatrix\n",
      "\u001b[0;31mIndexError\u001b[0m: index 7 is out of bounds for axis 0 with size 7"
     ]
    }
   ],
   "source": [
    "from elasticsearch_dsl import Index\n",
    "from collections import Counter\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# Creamos un nuevo indice vacio\n",
    "#index.delete()\n",
    "index = Index('ex5', using=client)\n",
    "\n",
    "# Leemos los documentos de la carpeta toy-docs\n",
    "folder_path = 'toy-docs'\n",
    "file_names = os.listdir(folder_path)\n",
    "file_names = sorted(file_names)\n",
    "documents = []\n",
    "for doc in file_names:\n",
    "    with open(os.path.join(folder_path, doc), 'r', encoding='utf-8') as f:\n",
    "        documents.append(f.read())\n",
    "\n",
    "# Añadimos los documentos al indice\n",
    "for t in documents:\n",
    "    client.index(index='ex5', document={'text': t})\n",
    "client.indices.refresh(index='ex5')\n",
    "\n",
    "word_counts1 = Counter()\n",
    "booleanMatrix = np.zeros((7,6))\n",
    "sc = scan(client, index='ex5', query={\"query\" : {\"match_all\": {}}})\n",
    "# Primero, construimos el vocabulario recorriendo todos los documentos\n",
    "vocab = []\n",
    "sc_vocab = scan(client, index='ex5', query={\"query\" : {\"match_all\": {}}})\n",
    "for s in sc_vocab:\n",
    "    tv = client.termvectors(index='ex5', id=s['_id'], fields=['text'], term_statistics=True, positions=False)\n",
    "    if 'text' in tv['term_vectors']:\n",
    "        for t in tv['term_vectors']['text']['terms']:\n",
    "            if t not in vocab:\n",
    "                vocab.append(t)\n",
    "\n",
    "# Ahora rellenamos la booleanMatrix\n",
    "sc_bool = scan(client, index='ex5', query={\"query\" : {\"match_all\": {}}})\n",
    "for doc_idx, s in enumerate(sc_bool):\n",
    "    tv = client.termvectors(index='ex5', id=s['_id'], fields=['text'], term_statistics=True, positions=False)\n",
    "    if 'text' in tv['term_vectors']:\n",
    "        for t in tv['term_vectors']['text']['terms']:\n",
    "            if t in vocab:\n",
    "                word_idx = vocab.index(t) - 1\n",
    "                booleanMatrix[doc_idx, word_idx] = 1\n",
    "booleanMatrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R2NSU5hAzyOe"
   },
   "source": [
    "## 6. Cleanup\n",
    "\n",
    "Finally, we remove the test index.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "j1NTaRy6zyOe",
    "outputId": "cc774e64-bbf7-49c5-aa25-ee7dae628f0e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ObjectApiResponse({'acknowledged': True})"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "CAIM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "nav_menu": {
    "height": "135px",
    "width": "252px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
