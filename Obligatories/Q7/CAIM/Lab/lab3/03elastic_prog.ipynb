{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fNRtdoFp0Exz"
   },
   "source": [
    "# CAIM Lab Session 3: Programming with Elastic Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MlfK9c9n0Ex1"
   },
   "source": [
    "In this session you will:\n",
    "\n",
    "- Learn how to tell ElasticSearch to apply different tokenizers and filters to the documents, like removing stopwords or stemming the words.\n",
    "- Study how these changes affect the terms that ElasticSearch puts in the index, and how this in turn affects searches.\n",
    "- Continuing previous work, implement tf-idf scheme over a repository of scietific article abstracts, including cosine measure for document similarities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S2in3mpq0Ex2"
   },
   "source": [
    "## 1. Preprocessing with ElasticSearch\n",
    "\n",
    "One of the tasks of the previous session was to remove from the documents vocabulary all those strings that were not proper words. Obviously this is a frequent task and all these kinds of DB have standard processes that help to filter and reduce the terms that are not useful for searching.\n",
    "\n",
    "Text, before being indexed, can be subjected to a pipeline of different processes that strips it from anything that will not be useful for a specific application. In ES these preprocessing pipelines are called _Analyzers_; ES includes many choices for each preprocessing step.\n",
    "\n",
    "\n",
    "The [following picture](https://www.elastic.co/es/blog/found-text-analysis-part-1) illustrates the chaining of preprocessing steps:\n",
    "\n",
    "![](https://api.contentstack.io/v2/assets/575e4c8c3dc542cb38c08267/download?uid=blt51e787daed39eae9?uid=blt51e787daed39eae9)\n",
    "\n",
    "The first step of the pipeline is usually a process that converts _raw text_ into _tokens_. We can for example tokenize a text using blanks and punctuation signs or use a language specific analyzer that detects words in an specific language or parse HTML/XML...\n",
    "\n",
    "[This section](https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-tokenizers.html) of the ElasticSearch manual explains the different text tokenizers available.\n",
    "\n",
    "Once we have obtained tokens, we can _normalize_ the strings and/or filter out valid tokens that are not useful. For instance, strings can be transformed to lowercase so all occurrences of the same word are mapped to the same token regardless of whether they were capitalized. Also, there are words that are not semantically useful when searching such as adverbs, articles or prepositions, in this case each language will have its own standard list of words; these are usually called \"_stopwords_\". Another language-specific token normalization is stemming. The stem of a word corresponds to the common part of a word from all variants are formed by inflection or addition of suffixes or prefixes. For instance, the words \"unstoppable\", \"stops\" and \"stopping\" all derive from the stem \"stop\". The idea is that all variations of a word will be represented by the same token.\n",
    "\n",
    "[This section](https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-tokenfilters.html) of ElasticSearch manual will give you an idea of the possibilities.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uxBaeVXh0Ex2"
   },
   "source": [
    "## 2. Modifying `ElasticSearch` index behavior (using Analyzers)\n",
    "\n",
    "In this section we are going to learn how to set up preprocessing with ElasticSearch. We are going to do it _inline_ so that you have a few examples and get familiar with how to set up ES analyzers. We are going to showcase the different options with the made up English phrase\n",
    "\n",
    "```\n",
    "My taylor 4ís was% &printing Printed rich the.\n",
    "```\n",
    "\n",
    "which contains symbols and weird things to see what effect the different tokenizers and filtering options have. We are going to work with three of the usual processes:\n",
    "\n",
    "* Tokenization\n",
    "* Normalization\n",
    "* Token filtering (stopwords and stemming)\n",
    "\n",
    "The next cells allow configuring the default tokenizer for an index and analyze an example text. We are going to play a little bit with the possibilities and see what tokens result from the analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "rYigfvIq0Ex3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(b'{\\n  \"name\" : \"10-192-170-215client.eduroam.upc.edu\",\\n  \"cluster_name\" : '\n",
      " b'\"elasticsearch\",\\n  \"cluster_uuid\" : \"nhigYoLMQYqHsoEar5DFlw\",\\n  \"version'\n",
      " b'\" : {\\n    \"number\" : \"9.1.4\",\\n    \"build_flavor\" : \"default\",\\n    \"build'\n",
      " b'_type\" : \"tar\",\\n    \"build_hash\" : \"0b7fe68d2e369469ff9e9f344ab6df64ab9c'\n",
      " b'5293\",\\n    \"build_date\" : \"2025-09-16T22:05:19.073893347Z\",\\n    \"build_s'\n",
      " b'napshot\" : false,\\n    \"lucene_version\" : \"10.2.2\",\\n    \"minimum_wire_com'\n",
      " b'patibility_version\" : \"8.19.0\",\\n    \"minimum_index_compatibility_version'\n",
      " b'\" : \"8.0.0\"\\n  },\\n  \"tagline\" : \"You Know, for Search\"\\n}\\n')\n"
     ]
    }
   ],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "from elasticsearch_dsl import Index, analyzer, tokenizer\n",
    "from elasticsearch.exceptions import NotFoundError\n",
    "from pprint import pprint\n",
    "import requests\n",
    "\n",
    "\n",
    "client = Elasticsearch(\"http://localhost:9200\", request_timeout=1000)\n",
    "\n",
    "try:\n",
    "    resp = requests.get('http://localhost:9200/')\n",
    "    pprint(resp.content)\n",
    "\n",
    "except Exception:\n",
    "    print('elasticsearch is not running')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "cDKA_vBB0Ex5",
    "outputId": "84aadef5-f58f-46ca-ed3f-61f2684ab742"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*** Using analyzer english_stem\n",
      "Tokens: ['my', 'taylor', '4í', 'print', 'print', 'rich']\n",
      "\n",
      "*** Using analyzer exact_match\n",
      "Tokens: ['my taylor 4ís was% &printing printed rich the.']\n",
      "\n",
      "*** Using analyzer whitespace_fold\n",
      "Tokens: ['My', 'taylor', '4is', 'was%', '&printing', 'Printed', 'rich', 'the.']\n"
     ]
    }
   ],
   "source": [
    "# Index analyzer cofiguration\n",
    "# Change the configuration and run this cell and the next to see the changes\n",
    "\n",
    "# work with dummy index called 'foo'\n",
    "ind = Index(\"foo\", using=client)\n",
    "\n",
    "# Drop existing index\n",
    "if ind.exists():\n",
    "    ind.delete()\n",
    "\n",
    "# Tokenizers: whitespace, standard, classic, letter\n",
    "# Filters: lowercase, asciifolding, stop, porter_stem, kstem, snowball\n",
    "ind.settings(\n",
    "    number_of_shards=1,\n",
    "    analysis={\n",
    "        \"analyzer\": {\n",
    "            \"english_stem\": {  # Analyzer with stemming for English\n",
    "                \"type\": \"custom\",\n",
    "                \"tokenizer\": \"standard\",\n",
    "                \"filter\": [\"lowercase\", \"stop\", \"porter_stem\"]\n",
    "            },\n",
    "            \"exact_match\": {   # Analyzer that preserves terms\n",
    "                \"type\": \"custom\",\n",
    "                \"tokenizer\": \"keyword\",\n",
    "                \"filter\": [\"lowercase\"]\n",
    "            },\n",
    "            \"whitespace_fold\": {  # Analyzer splitting on whitespace and folding accents\n",
    "                \"type\": \"custom\",\n",
    "                \"tokenizer\": \"whitespace\",\n",
    "                \"filter\": [\"asciifolding\"]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    ")\n",
    "ind.create()\n",
    "\n",
    "# now you can ask the index to analyze any text, feel free to change the text\n",
    "\n",
    "#res = ind.analyze({'text':u'my taylor 4ís was% &printing printed rich the.'})\n",
    "for prep_type in ['english_stem', 'exact_match', 'whitespace_fold']:\n",
    "    print(f'\\n*** Using analyzer {prep_type}')\n",
    "    res = client.indices.analyze(\n",
    "        index=\"foo\",\n",
    "        analyzer=prep_type,\n",
    "        text=\"My taylor 4ís was% &printing Printed rich the.\"\n",
    "    )\n",
    "\n",
    "    print(\"Tokens:\", [t[\"token\"] for t in res[\"tokens\"]])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6xigh1aA0EyA"
   },
   "source": [
    "---\n",
    "\n",
    "**Exercise 1:** solve exercise 1 from problem set 1 using ElasticSearch. You can use the following string.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*** Using analyzer english_stem\n",
      "Tokens: ['we', 'found', 'my', 'ladi', 'light', 'room', 'read', 'lamp', 'shade', 'screw', 'down', 'so', 'over', 'shadow', 'her', 'face', 'instead', 'look', 'up', 'us', 'her', 'usual', 'straightforward', 'wai', 'she', 'sat', 'close', 'tabl', 'kept', 'her', 'ey', 'fix', 'obstin', 'open', 'book', 'offic', 'she', 'said', 'import', 'inquiri', 'you', 'conduct', 'know', 'beforehand', 'ani', 'person', 'now', 'hous', 'wish', 'leav']\n",
      "\n",
      "*** Using analyzer exact_match\n",
      "Tokens: ['\\nwe found my lady with no light in the room but the reading-lamp.\\nthe shade was screwed down so as to over-shadow her face. instead of looking up at us in her usual straightforward way, she sat\\nclose at the table, and kept her eyes fixed obstinately on an open\\nbook.\\n“officer,” she said, “it is important to the inquiry you are conducting to know beforehand if any person now in this house wishes\\nto leave it?”\\n']\n",
      "\n",
      "*** Using analyzer whitespace_fold\n",
      "Tokens: ['We', 'found', 'my', 'lady', 'with', 'no', 'light', 'in', 'the', 'room', 'but', 'the', 'reading-lamp.', 'The', 'shade', 'was', 'screwed', 'down', 'so', 'as', 'to', 'over-shadow', 'her', 'face.', 'Instead', 'of', 'looking', 'up', 'at', 'us', 'in', 'her', 'usual', 'straightforward', 'way,', 'she', 'sat', 'close', 'at', 'the', 'table,', 'and', 'kept', 'her', 'eyes', 'fixed', 'obstinately', 'on', 'an', 'open', 'book.', '\"Officer,\"', 'she', 'said,', '\"it', 'is', 'important', 'to', 'the', 'inquiry', 'you', 'are', 'conducting', 'to', 'know', 'beforehand', 'if', 'any', 'person', 'now', 'in', 'this', 'house', 'wishes', 'to', 'leave', 'it?\"']\n"
     ]
    }
   ],
   "source": [
    "moonstone = \"\"\"\n",
    "We found my lady with no light in the room but the reading-lamp.\n",
    "The shade was screwed down so as to over-shadow her face. Instead of looking up at us in her usual straightforward way, she sat\n",
    "close at the table, and kept her eyes fixed obstinately on an open\n",
    "book.\n",
    "“Officer,” she said, “it is important to the inquiry you are conducting to know beforehand if any person now in this house wishes\n",
    "to leave it?”\n",
    "\"\"\"\n",
    "\n",
    "ind = Index(\"ex1\", using=client)\n",
    "\n",
    "if ind.exists():\n",
    "    ind.delete()\n",
    "\n",
    "ind.settings(\n",
    "    number_of_shards=1,\n",
    "    analysis={\n",
    "        \"analyzer\": {\n",
    "            \"english_stem\": {  # Analyzer with stemming for English\n",
    "                \"type\": \"custom\",\n",
    "                \"tokenizer\": \"standard\",\n",
    "                \"filter\": [\"lowercase\", \"stop\", \"porter_stem\"]\n",
    "            },\n",
    "            \"exact_match\": {   # Analyzer that preserves terms\n",
    "                \"type\": \"custom\",\n",
    "                \"tokenizer\": \"keyword\",\n",
    "                \"filter\": [\"lowercase\"]\n",
    "            },\n",
    "            \"whitespace_fold\": {  # Analyzer splitting on whitespace and folding accents\n",
    "                \"type\": \"custom\",\n",
    "                \"tokenizer\": \"whitespace\",\n",
    "                \"filter\": [\"asciifolding\"]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "ind.create()\n",
    "\n",
    "# now you can ask the index to analyze any text, feel free to change the text\n",
    "\n",
    "#res = ind.analyze({'text':u'my taylor 4ís was% &printing printed rich the.'})\n",
    "for prep_type in ['english_stem', 'exact_match', 'whitespace_fold']:\n",
    "    print(f'\\n*** Using analyzer {prep_type}')\n",
    "    res = client.indices.analyze(\n",
    "        index=\"foo\",\n",
    "        analyzer=prep_type,\n",
    "        text=moonstone\n",
    "    )\n",
    "\n",
    "    print(\"Tokens:\", [t[\"token\"] for t in res[\"tokens\"]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como podemos observar en los resultados podemos ver que el whitespace_fold es el que nos da el resultado más parecido al que obtuvimos en el set de problemas 1. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6C-mx3t_0EyB"
   },
   "source": [
    "## 3. Indexing script `IndexFilesPreprocess.py`\n",
    "\n",
    "You should study how the provided indexer script named `IndexFilesPreprocess.py` works.\n",
    "Its usage is as follows:\n",
    "\n",
    "```\n",
    "usage: IndexFilesPreprocess.py [-h] --path PATH --index INDEX\n",
    "                               [--token {standard,whitespace,classic,letter}]\n",
    "                               [--filter ...]\n",
    "\n",
    "optional arguments:\n",
    "  -h, --help            show this help message and exit\n",
    "  --path PATH           Path to the files\n",
    "  --index INDEX         Index for the files\n",
    "  --token {standard,whitespace,classic,letter}\n",
    "                        Text tokenizer\n",
    "  --filter ...          Text filter: lowercase, asciifolding, stop,\n",
    "                        porter_stem, kstem, snowball\n",
    "```\n",
    "\n",
    "So, you can pass a `--path` argument which is the path to a directory where the files that you want to index are located (possibly in subdirectories);\n",
    "you can specify through `--index` the name of the index to be created; you can also specify the _tokenization_ procedure to be used with the `--token` argument;\n",
    "and finally you can apply preprocessing filters through the `--filter` argument. As an example call,\n",
    "\n",
    "```\n",
    "$ python3 IndexFilesPreprocess.py --index toy --path toy-docs --token letter --filter lowercase asciifolding\n",
    "```\n",
    "\n",
    "would create an index called `toy` adding all files located within the subdirectory `toy-docs`, applying the letter tokenizer and applying `lowercase` and `asciifolding` preprocessing.\n",
    "\n",
    "\n",
    "In particular, you should pay attention to:\n",
    "\n",
    "- how preprocessing is done within the script\n",
    "- how the `bulk` operation is used for adding documents to the index (instead of adding files one-by-one)\n",
    "- the structure of documents added, which contains a `text` field with the content but also a `path` field with the name of the file being added"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MGT5lyNG0EyB"
   },
   "source": [
    "\n",
    "\n",
    "## 4. Coding exercises\n",
    "\n",
    "---\n",
    "\n",
    "**Exercise 2:**  \n",
    "\n",
    "Download the `arxiv_abs.zip` repository from `https://www.cs.upc.edu/~marias/arxiv_abs.zip`; unzip it. You should see a directory containing folders that contain\n",
    "text files. These correspond to abstracts of scientific papers in several topics from the [arXiv.org](https://arxiv.org) repository. Index these abstracts using the `IndexFilesPreprocess.py` script (be patient, it takes a while). Double check that your index contains around 58K documents. Pay special attention to how file names are stored in the `path` field of the indexed elasticsearch documents.\n",
    "\n",
    "---\n",
    "\n",
    "**Exercise 3:**\n",
    "\n",
    "Write a function that computes the _cosine similarity_ between pairs of documents in your index. For that, you will find useful the computations from last week that computed the _tf-idf_ vectors of documents in the toy-document dataset. It is important to use _sparse representation_ for these vectors, either through the use of a python dictionary (with `term: weight` entries), or alternatively you could use a list of pairs `(term, weight)`; if you choose the latter, then it is going to be useful to sort the lists by term so that you can find common terms in order to compute the similarities.\n",
    "\n",
    "\n",
    "_Hint: the `termvector` function that we saw in the last lab session also returns (corpus) document frequencies that you need in order to compute the idf part of the weights:_ \n",
    "\n",
    "- `tv['term_vectors']['text']['terms'][t]['term_freq']`\n",
    "        this gives you the frequency of term t within the document\n",
    "\n",
    "- `tv['term_vectors']['text']['terms'][t]['doc_freq']`\n",
    "        this gives you the document frequency (nr. of docs containing `t`) of term t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from random import sample\n",
    "from elasticsearch.helpers import scan\n",
    "\n",
    "file_path = \"arxiv\"\n",
    "list_dirs = os.listdir(file_path)\n",
    "list_files = []\n",
    "aux = []\n",
    "for dir_name in list_dirs:\n",
    "    dir_path = os.path.join(file_path, dir_name)\n",
    "    if os.path.isdir(dir_path):\n",
    "        for filename in os.listdir(dir_path):\n",
    "            full_path = os.path.join(dir_name, filename)\n",
    "            aux.append(full_path)\n",
    "    list_files.append(sample(aux, 50))\n",
    "    aux = []\n",
    "    \n",
    "# número de documentos\n",
    "files = [f for sub in list_files for f in sub]\n",
    "print(files)\n",
    "print(len(files))\n",
    "\n",
    "index = Index('ex3', using=client)\n",
    "for filename in files:\n",
    "    path = os.path.join(file_path, filename)\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        text = f.read()\n",
    "        client.index(index='ex3', document={'text': text, 'path': path})\n",
    "        \n",
    "client.indices.refresh(index='ex3')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# diccionario: d{id} -> {word: tf-idf}\n",
    "tfidf_table = defaultdict(dict)\n",
    "\n",
    "sc = scan(client, index='ex3', query={\"query\" : {\"match_all\": {}}})\n",
    "for i, s in enumerate(sc):\n",
    "    tv = client.termvectors(\n",
    "        index='ex3', id=s['_id'],\n",
    "        fields=['text'], term_statistics=True, positions=False\n",
    "    )\n",
    "    if 'text' in tv['term_vectors']:\n",
    "        terms = tv['term_vectors']['text']['terms']\n",
    "        for word, stats in terms.items():\n",
    "            tf = stats['term_freq']\n",
    "            df = stats['doc_freq']\n",
    "            # usamos la variación smooth idf, que evita la división por \n",
    "            # cero y modera los pesos de los terminos raros.\n",
    "            idf = math.log(D / (1 + df)) + 1\n",
    "            tfidf = tf * idf\n",
    "            tfidf_table[f\"d{i+1}\"][word] = tfidf\n",
    "\n",
    "#index.delete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Exercise 4:**\n",
    "\n",
    "Finally, using your code above, build a matrix that reflects the average cosine similarities between pairs of documents in different paper abstract categories. These categories are reflected in the path names of the files, e.g. in my computer, the path name to abstract `/tmp/arxiv/hep-ph.updates.on.arXiv.org/000787` corresponds to the category of `hep-ph` papers. The categories are `astro-ph, cs, hep-th, physics, cond-mat, hep-ph, math, quant-ph`, which can be extracted from path names.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RXIST0ib0EyB"
   },
   "source": [
    "Finally, the following piece of code may be useful to see the content of a few random documents within an index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YpBoV4Hp0EyB",
    "outputId": "0b7d40dd-eee0-49b8-a369-4468fea7fdba"
   },
   "outputs": [],
   "source": [
    "def print_docs_from_index(index_name, client, max_docs):\n",
    "\n",
    "    print(f\"===================\")\n",
    "    info = client.cat.count(index=index_name, format = \"json\")[0]\n",
    "    print(f\"Index: {index_name} with {info['count']} documents.\")\n",
    "    print()\n",
    "\n",
    "    res = client.search(index=index_name, size = max_docs, query= {'match_all' : {}})\n",
    "\n",
    "    for doc in res['hits']['hits']:\n",
    "        print (doc['_id'], doc['_source'])\n",
    "\n",
    "print_docs_from_index('arxiv', Elasticsearch(\"http://localhost:9200\", request_timeout=1000), max_docs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "crk-hI4I0EyC"
   },
   "source": [
    "---\n",
    "\n",
    "**Exercise 5: (may take time..)**\n",
    "\n",
    "Can you find duplicate documents in the corpus provided?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "CAIM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "nav_menu": {
    "height": "81px",
    "width": "252px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
