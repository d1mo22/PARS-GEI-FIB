{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fNRtdoFp0Exz"
   },
   "source": [
    "# CAIM Lab Session 3: Programming with Elastic Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MlfK9c9n0Ex1"
   },
   "source": [
    "In this session you will:\n",
    "\n",
    "- Learn how to tell ElasticSearch to apply different tokenizers and filters to the documents, like removing stopwords or stemming the words.\n",
    "- Study how these changes affect the terms that ElasticSearch puts in the index, and how this in turn affects searches.\n",
    "- Continuing previous work, implement tf-idf scheme over a repository of scietific article abstracts, including cosine measure for document similarities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S2in3mpq0Ex2"
   },
   "source": [
    "## 1. Preprocessing with ElasticSearch\n",
    "\n",
    "One of the tasks of the previous session was to remove from the documents vocabulary all those strings that were not proper words. Obviously this is a frequent task and all these kinds of DB have standard processes that help to filter and reduce the terms that are not useful for searching.\n",
    "\n",
    "Text, before being indexed, can be subjected to a pipeline of different processes that strips it from anything that will not be useful for a specific application. In ES these preprocessing pipelines are called _Analyzers_; ES includes many choices for each preprocessing step.\n",
    "\n",
    "\n",
    "The [following picture](https://www.elastic.co/es/blog/found-text-analysis-part-1) illustrates the chaining of preprocessing steps:\n",
    "\n",
    "![](https://api.contentstack.io/v2/assets/575e4c8c3dc542cb38c08267/download?uid=blt51e787daed39eae9?uid=blt51e787daed39eae9)\n",
    "\n",
    "The first step of the pipeline is usually a process that converts _raw text_ into _tokens_. We can for example tokenize a text using blanks and punctuation signs or use a language specific analyzer that detects words in an specific language or parse HTML/XML...\n",
    "\n",
    "[This section](https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-tokenizers.html) of the ElasticSearch manual explains the different text tokenizers available.\n",
    "\n",
    "Once we have obtained tokens, we can _normalize_ the strings and/or filter out valid tokens that are not useful. For instance, strings can be transformed to lowercase so all occurrences of the same word are mapped to the same token regardless of whether they were capitalized. Also, there are words that are not semantically useful when searching such as adverbs, articles or prepositions, in this case each language will have its own standard list of words; these are usually called \"_stopwords_\". Another language-specific token normalization is stemming. The stem of a word corresponds to the common part of a word from all variants are formed by inflection or addition of suffixes or prefixes. For instance, the words \"unstoppable\", \"stops\" and \"stopping\" all derive from the stem \"stop\". The idea is that all variations of a word will be represented by the same token.\n",
    "\n",
    "[This section](https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-tokenfilters.html) of ElasticSearch manual will give you an idea of the possibilities.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uxBaeVXh0Ex2"
   },
   "source": [
    "## 2. Modifying `ElasticSearch` index behavior (using Analyzers)\n",
    "\n",
    "In this section we are going to learn how to set up preprocessing with ElasticSearch. We are going to do it _inline_ so that you have a few examples and get familiar with how to set up ES analyzers. We are going to showcase the different options with the made up English phrase\n",
    "\n",
    "```\n",
    "My taylor 4ís was% &printing Printed rich the.\n",
    "```\n",
    "\n",
    "which contains symbols and weird things to see what effect the different tokenizers and filtering options have. We are going to work with three of the usual processes:\n",
    "\n",
    "* Tokenization\n",
    "* Normalization\n",
    "* Token filtering (stopwords and stemming)\n",
    "\n",
    "The next cells allow configuring the default tokenizer for an index and analyze an example text. We are going to play a little bit with the possibilities and see what tokens result from the analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "rYigfvIq0Ex3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(b'{\\n  \"name\" : \"10-192-204-88client.eduroam.upc.edu\",\\n  \"cluster_name\" : \"'\n",
      " b'elasticsearch\",\\n  \"cluster_uuid\" : \"nhigYoLMQYqHsoEar5DFlw\",\\n  \"version\"'\n",
      " b' : {\\n    \"number\" : \"9.1.4\",\\n    \"build_flavor\" : \"default\",\\n    \"build_'\n",
      " b'type\" : \"tar\",\\n    \"build_hash\" : \"0b7fe68d2e369469ff9e9f344ab6df64ab9c5'\n",
      " b'293\",\\n    \"build_date\" : \"2025-09-16T22:05:19.073893347Z\",\\n    \"build_sn'\n",
      " b'apshot\" : false,\\n    \"lucene_version\" : \"10.2.2\",\\n    \"minimum_wire_comp'\n",
      " b'atibility_version\" : \"8.19.0\",\\n    \"minimum_index_compatibility_version\"'\n",
      " b' : \"8.0.0\"\\n  },\\n  \"tagline\" : \"You Know, for Search\"\\n}\\n')\n"
     ]
    }
   ],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "from elasticsearch_dsl import Index, analyzer, tokenizer\n",
    "from elasticsearch.exceptions import NotFoundError\n",
    "from pprint import pprint\n",
    "import requests\n",
    "\n",
    "\n",
    "client = Elasticsearch(\"http://localhost:9200\", request_timeout=1000)\n",
    "\n",
    "try:\n",
    "    resp = requests.get('http://localhost:9200/')\n",
    "    pprint(resp.content)\n",
    "\n",
    "except Exception:\n",
    "    print('elasticsearch is not running')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "cDKA_vBB0Ex5",
    "outputId": "84aadef5-f58f-46ca-ed3f-61f2684ab742"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*** Using analyzer english_stem\n",
      "Tokens: ['my', 'taylor', '4í', 'print', 'print', 'rich']\n",
      "\n",
      "*** Using analyzer exact_match\n",
      "Tokens: ['my taylor 4ís was% &printing printed rich the.']\n",
      "\n",
      "*** Using analyzer whitespace_fold\n",
      "Tokens: ['My', 'taylor', '4is', 'was%', '&printing', 'Printed', 'rich', 'the.']\n"
     ]
    }
   ],
   "source": [
    "# Index analyzer cofiguration\n",
    "# Change the configuration and run this cell and the next to see the changes\n",
    "\n",
    "# work with dummy index called 'foo'\n",
    "ind = Index(\"foo\", using=client)\n",
    "\n",
    "# Drop existing index\n",
    "if ind.exists():\n",
    "    ind.delete()\n",
    "\n",
    "# Tokenizers: whitespace, standard, classic, letter\n",
    "# Filters: lowercase, asciifolding, stop, porter_stem, kstem, snowball\n",
    "ind.settings(\n",
    "    number_of_shards=1,\n",
    "    analysis={\n",
    "        \"analyzer\": {\n",
    "            \"english_stem\": {  # Analyzer with stemming for English\n",
    "                \"type\": \"custom\",\n",
    "                \"tokenizer\": \"standard\",\n",
    "                \"filter\": [\"lowercase\", \"stop\", \"porter_stem\"]\n",
    "            },\n",
    "            \"exact_match\": {   # Analyzer that preserves terms\n",
    "                \"type\": \"custom\",\n",
    "                \"tokenizer\": \"keyword\",\n",
    "                \"filter\": [\"lowercase\"]\n",
    "            },\n",
    "            \"whitespace_fold\": {  # Analyzer splitting on whitespace and folding accents\n",
    "                \"type\": \"custom\",\n",
    "                \"tokenizer\": \"whitespace\",\n",
    "                \"filter\": [\"asciifolding\"]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    ")\n",
    "ind.create()\n",
    "\n",
    "# now you can ask the index to analyze any text, feel free to change the text\n",
    "\n",
    "#res = ind.analyze({'text':u'my taylor 4ís was% &printing printed rich the.'})\n",
    "for prep_type in ['english_stem', 'exact_match', 'whitespace_fold']:\n",
    "    print(f'\\n*** Using analyzer {prep_type}')\n",
    "    res = client.indices.analyze(\n",
    "        index=\"foo\",\n",
    "        analyzer=prep_type,\n",
    "        text=\"My taylor 4ís was% &printing Printed rich the.\"\n",
    "    )\n",
    "\n",
    "    print(\"Tokens:\", [t[\"token\"] for t in res[\"tokens\"]])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6xigh1aA0EyA"
   },
   "source": [
    "---\n",
    "\n",
    "**Exercise 1:** solve exercise 1 from problem set 1 using ElasticSearch. You can use the following string.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*** Using analyzer english_stem\n",
      "Tokens: ['we', 'found', 'my', 'ladi', 'light', 'room', 'read', 'lamp', 'shade', 'screw', 'down', 'so', 'over', 'shadow', 'her', 'face', 'instead', 'look', 'up', 'us', 'her', 'usual', 'straightforward', 'wai', 'she', 'sat', 'close', 'tabl', 'kept', 'her', 'ey', 'fix', 'obstin', 'open', 'book', 'offic', 'she', 'said', 'import', 'inquiri', 'you', 'conduct', 'know', 'beforehand', 'ani', 'person', 'now', 'hous', 'wish', 'leav']\n",
      "\n",
      "*** Using analyzer exact_match\n",
      "Tokens: ['\\nwe found my lady with no light in the room but the reading-lamp.\\nthe shade was screwed down so as to over-shadow her face. instead of looking up at us in her usual straightforward way, she sat\\nclose at the table, and kept her eyes fixed obstinately on an open\\nbook.\\n“officer,” she said, “it is important to the inquiry you are conducting to know beforehand if any person now in this house wishes\\nto leave it?”\\n']\n",
      "\n",
      "*** Using analyzer whitespace_fold\n",
      "Tokens: ['We', 'found', 'my', 'lady', 'with', 'no', 'light', 'in', 'the', 'room', 'but', 'the', 'reading-lamp.', 'The', 'shade', 'was', 'screwed', 'down', 'so', 'as', 'to', 'over-shadow', 'her', 'face.', 'Instead', 'of', 'looking', 'up', 'at', 'us', 'in', 'her', 'usual', 'straightforward', 'way,', 'she', 'sat', 'close', 'at', 'the', 'table,', 'and', 'kept', 'her', 'eyes', 'fixed', 'obstinately', 'on', 'an', 'open', 'book.', '\"Officer,\"', 'she', 'said,', '\"it', 'is', 'important', 'to', 'the', 'inquiry', 'you', 'are', 'conducting', 'to', 'know', 'beforehand', 'if', 'any', 'person', 'now', 'in', 'this', 'house', 'wishes', 'to', 'leave', 'it?\"']\n"
     ]
    }
   ],
   "source": [
    "moonstone = \"\"\"\n",
    "We found my lady with no light in the room but the reading-lamp.\n",
    "The shade was screwed down so as to over-shadow her face. Instead of looking up at us in her usual straightforward way, she sat\n",
    "close at the table, and kept her eyes fixed obstinately on an open\n",
    "book.\n",
    "“Officer,” she said, “it is important to the inquiry you are conducting to know beforehand if any person now in this house wishes\n",
    "to leave it?”\n",
    "\"\"\"\n",
    "\n",
    "ind = Index(\"ex1\", using=client)\n",
    "\n",
    "if ind.exists():\n",
    "    ind.delete()\n",
    "\n",
    "ind.settings(\n",
    "    number_of_shards=1,\n",
    "    analysis={\n",
    "        \"analyzer\": {\n",
    "            \"english_stem\": {  # Analyzer with stemming for English\n",
    "                \"type\": \"custom\",\n",
    "                \"tokenizer\": \"standard\",\n",
    "                \"filter\": [\"lowercase\", \"stop\", \"porter_stem\"]\n",
    "            },\n",
    "            \"exact_match\": {   # Analyzer that preserves terms\n",
    "                \"type\": \"custom\",\n",
    "                \"tokenizer\": \"keyword\",\n",
    "                \"filter\": [\"lowercase\"]\n",
    "            },\n",
    "            \"whitespace_fold\": {  # Analyzer splitting on whitespace and folding accents\n",
    "                \"type\": \"custom\",\n",
    "                \"tokenizer\": \"whitespace\",\n",
    "                \"filter\": [\"asciifolding\"]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "ind.create()\n",
    "\n",
    "# now you can ask the index to analyze any text, feel free to change the text\n",
    "\n",
    "#res = ind.analyze({'text':u'my taylor 4ís was% &printing printed rich the.'})\n",
    "for prep_type in ['english_stem', 'exact_match', 'whitespace_fold']:\n",
    "    print(f'\\n*** Using analyzer {prep_type}')\n",
    "    res = client.indices.analyze(\n",
    "        index=\"foo\",\n",
    "        analyzer=prep_type,\n",
    "        text=moonstone\n",
    "    )\n",
    "\n",
    "    print(\"Tokens:\", [t[\"token\"] for t in res[\"tokens\"]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como podemos observar en los resultados podemos ver que el whitespace_fold es el que nos da el resultado más parecido al que obtuvimos en el set de problemas 1. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6C-mx3t_0EyB"
   },
   "source": [
    "## 3. Indexing script `IndexFilesPreprocess.py`\n",
    "\n",
    "You should study how the provided indexer script named `IndexFilesPreprocess.py` works.\n",
    "Its usage is as follows:\n",
    "\n",
    "```\n",
    "usage: IndexFilesPreprocess.py [-h] --path PATH --index INDEX\n",
    "                               [--token {standard,whitespace,classic,letter}]\n",
    "                               [--filter ...]\n",
    "\n",
    "optional arguments:\n",
    "  -h, --help            show this help message and exit\n",
    "  --path PATH           Path to the files\n",
    "  --index INDEX         Index for the files\n",
    "  --token {standard,whitespace,classic,letter}\n",
    "                        Text tokenizer\n",
    "  --filter ...          Text filter: lowercase, asciifolding, stop,\n",
    "                        porter_stem, kstem, snowball\n",
    "```\n",
    "\n",
    "So, you can pass a `--path` argument which is the path to a directory where the files that you want to index are located (possibly in subdirectories);\n",
    "you can specify through `--index` the name of the index to be created; you can also specify the _tokenization_ procedure to be used with the `--token` argument;\n",
    "and finally you can apply preprocessing filters through the `--filter` argument. As an example call,\n",
    "\n",
    "```\n",
    "$ python3 IndexFilesPreprocess.py --index toy --path toy-docs --token letter --filter lowercase asciifolding\n",
    "```\n",
    "\n",
    "would create an index called `toy` adding all files located within the subdirectory `toy-docs`, applying the letter tokenizer and applying `lowercase` and `asciifolding` preprocessing.\n",
    "\n",
    "\n",
    "In particular, you should pay attention to:\n",
    "\n",
    "- how preprocessing is done within the script\n",
    "- how the `bulk` operation is used for adding documents to the index (instead of adding files one-by-one)\n",
    "- the structure of documents added, which contains a `text` field with the content but also a `path` field with the name of the file being added"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MGT5lyNG0EyB"
   },
   "source": [
    "\n",
    "\n",
    "## 4. Coding exercises\n",
    "\n",
    "---\n",
    "\n",
    "**Exercise 2:**  \n",
    "\n",
    "Download the `arxiv_abs.zip` repository from `https://www.cs.upc.edu/~marias/arxiv_abs.zip`; unzip it. You should see a directory containing folders that contain\n",
    "text files. These correspond to abstracts of scientific papers in several topics from the [arXiv.org](https://arxiv.org) repository. Index these abstracts using the `IndexFilesPreprocess.py` script (be patient, it takes a while). Double check that your index contains around 58K documents. Pay special attention to how file names are stored in the `path` field of the indexed elasticsearch documents.\n",
    "\n",
    "---\n",
    "\n",
    "**Exercise 3:**\n",
    "\n",
    "Write a function that computes the _cosine similarity_ between pairs of documents in your index. For that, you will find useful the computations from last week that computed the _tf-idf_ vectors of documents in the toy-document dataset. It is important to use _sparse representation_ for these vectors, either through the use of a python dictionary (with `term: weight` entries), or alternatively you could use a list of pairs `(term, weight)`; if you choose the latter, then it is going to be useful to sort the lists by term so that you can find common terms in order to compute the similarities.\n",
    "\n",
    "\n",
    "_Hint: the `termvector` function that we saw in the last lab session also returns (corpus) document frequencies that you need in order to compute the idf part of the weights:_ \n",
    "\n",
    "- `tv['term_vectors']['text']['terms'][t]['term_freq']`\n",
    "        this gives you the frequency of term t within the document\n",
    "\n",
    "- `tv['term_vectors']['text']['terms'][t]['doc_freq']`\n",
    "        this gives you the document frequency (nr. of docs containing `t`) of term t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58102\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from random import sample\n",
    "\n",
    "file_path = \"arxiv\"\n",
    "list_dirs = os.listdir(file_path)\n",
    "list_files = []\n",
    "aux = []\n",
    "for dir_name in list_dirs:\n",
    "    dir_path = os.path.join(file_path, dir_name)\n",
    "    if os.path.isdir(dir_path):\n",
    "        for filename in os.listdir(dir_path):\n",
    "            full_path = os.path.join(dir_name, filename)\n",
    "            aux.append(full_path)\n",
    "    list_files.append(sample(aux, 200))\n",
    "    aux = []\n",
    "# número de documentos\n",
    "print(list_files)\n",
    "D = len(list_files)\n",
    "print(D)\n",
    "\n",
    "index = Index('ex3', using=client) \n",
    "\n",
    "for filename in list_files:\n",
    "    with open(os.path.join(file_path, filename), \"r\", encoding=\"utf-8\") as f:\n",
    "        text = f.read()\n",
    "        client.index(index='ex3', document={'text': text})\n",
    "        \n",
    "client.indices.refresh(index='ex3')\n",
    "\n",
    "# diccionario: d{id} -> {word: tf-idf}\n",
    "tfidf_table = defaultdict(dict)\n",
    "\n",
    "sc = scan(client, index='ex3', query={\"query\" : {\"match_all\": {}}})\n",
    "for i, s in enumerate(sc):\n",
    "    tv = client.termvectors(\n",
    "        index='ex3', id=s['_id'],\n",
    "        fields=['text'], term_statistics=True, positions=False\n",
    "    )\n",
    "    if 'text' in tv['term_vectors']:\n",
    "        terms = tv['term_vectors']['text']['terms']\n",
    "        for word, stats in terms.items():\n",
    "            tf = stats['term_freq']\n",
    "            df = stats['doc_freq']\n",
    "            # usamos la variación smooth idf, que evita la división por \n",
    "            # cero y modera los pesos de los terminos raros.\n",
    "            idf = math.log(D / (1 + df)) + 1\n",
    "            tfidf = tf * idf\n",
    "            tfidf_table[f\"d{i+1}\"][word] = tfidf\n",
    "\n",
    "# convertir a DataFrame para visualizar\n",
    "df = pd.DataFrame(tfidf_table).fillna(0)\n",
    "print(df)\n",
    "\n",
    "index.delete()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Exercise 4:**\n",
    "\n",
    "Finally, using your code above, build a matrix that reflects the average cosine similarities between pairs of documents in different paper abstract categories. These categories are reflected in the path names of the files, e.g. in my computer, the path name to abstract `/tmp/arxiv/hep-ph.updates.on.arXiv.org/000787` corresponds to the category of `hep-ph` papers. The categories are `astro-ph, cs, hep-th, physics, cond-mat, hep-ph, math, quant-ph`, which can be extracted from path names.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RXIST0ib0EyB"
   },
   "source": [
    "Finally, the following piece of code may be useful to see the content of a few random documents within an index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "YpBoV4Hp0EyB",
    "outputId": "0b7d40dd-eee0-49b8-a369-4468fea7fdba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================\n",
      "Index: arxiv with 58102 documents.\n",
      "\n",
      "1 {'path': '/tmp/arxiv/hep-ph.updates.on.arXiv.org/000787', 'text': \"A set of eight self-consistent, time-dependent supernova (SN) simulations in three spatial dimensions (3D) for 9 solar-mass and 20 solar-mass progenitors is evaluated for the presence of dipolar asymmetries of the electron lepton-number emission as discovered by Tamborra et al. and termed lepton-number emission self-sustained asymmetry (LESA). The simulations were performed with the Aenus-Alcar neutrino/hydrodynamics code, which treats the energy- and velocity-dependent transport of neutrinos of all flavors by a two-moment scheme with algebraic M1 closure. For each of the progenitors, results with fully multi-dimensional (FMD) neutrino transport and with ray-by-ray-plus (RbR+) approximation are considered for two different grid resolutions. While the 9 solar-mass models develop explosions, the 20 solar-mass progenitor does not explode with the employed version of simplified neutrino opacities. In all 3D models we observe the growth of substantial dipole amplitudes of the lepton-number (electron neutrino minus antineutrino) flux with stable or slowly time-evolving direction and overall properties fully consistent with the LESA phenomenon. Models with RbR+ transport develop LESA dipoles somewhat faster and with temporarily higher amplitudes, but the FMD calculations exhibit cleaner hemispheric asymmetries with a far more dominant dipole. In contrast, the RbR+ results display much wider multipole spectra of the neutrino-emission anisotropies with significant power also in the quadrupole and higher-order modes. Our results disprove speculations that LESA is a numerical artifact of RbR+ transport. We also discuss LESA as consequence of a dipolar convection flow inside of the nascent neutron star and establish, tentatively, a connection to Chandrasekhar's linear theory of thermal instability in spherical shells.\"}\n",
      "2 {'path': '/tmp/arxiv/hep-ph.updates.on.arXiv.org/001655', 'text': 'We review the current understanding of heavy quark parton distributions in nucleons and their impact on deep inelastic scattering, collider physics, and other processes at high energies. The determination of the heavy-quark parton distribution functions is particularly significant for the analysis of hard processes at LHC energies, including the forward rapidity high $x_\\\\mathrm{F}$ domain. The contribution of \"intrinsic\" heavy quarks, which are multiply connected to the valence quarks of nucleons, is reviewed within non-perturbative physics which provides new information on the fundamental structure of hadrons in QCD. A new prediction for the non-perturbative intrinsic charm-anticharm asymmetry of the proton eigenstate has recently been obtained from a QCD lattice gauge theory calculation of the proton\\'s $G_\\\\mathrm{E}^p(Q^2)$ form factor. This form factor only arises from non-valence quarks and anti-quarks if they have different contributions in the proton\\'s eigenstate. This result, together with the exclusive and inclusive connection and analytic constraints on the form of hadronic structure functions from Light-Front Holographic QCD (LFHQCD) predicts a significant non-perturbative $c(x,Q) - \\\\bar{c}(x,Q)$ asymmetry in the proton structure function at high $x$, consistent with the dynamics predicted by intrinsic charm models. Recent ATLAS data on the associated production of prompt photons and charm-quark jets in $pp$ collisions at $\\\\sqrt{s} = 8$ TeV has provided new constraints on non-perturbative intrinsic charm and tests of the LGTH predictions. We also focus on other experimental observables which have high sensitivity to the intrinsic heavy contributions to PDFs.'}\n",
      "3 {'path': '/tmp/arxiv/hep-ph.updates.on.arXiv.org/001467', 'text': 'The nature of Milky Way dwarf spheroidals (MW dSphs) has been questioned, in particular whether they are dominated by dark matter (DM). Here we investigate an alternative scenario, for which tidal shocks are exerted by the MW to DM-free dSphs after a first infall of their gas-rich progenitors, and for which theoretical calculations have been verified by pure N-body simulations. Whether or not the dSphs are on their first infall cannot be resolved on the sole basis of their star formation history. In fact, gas removal may cause complex gravitational instabilities and near-pericenter passages can give rise to tidal disruptive processes. Advanced precision with the Gaia satellite in determining both their past orbital motions and the MW velocity curve is, however, providing crucial results. First, tidal shocks explain why DM-free dSphs are found preferentially near their pericenter, where they are in a destructive process, while their chance to be long-lived satellites is associated with a very low probability P~ 2 10^-7, which is at odds with the current DM-dominated dSph scenario. Second, most dSph binding energies are consistent with a first infall. Third, the MW tidal shocks that predict the observed dSph velocity dispersions are themselves predicted in amplitude by the most accurate MW velocity curve. Fourth, tidal shocks accurately predict the forces or accelerations exerted at half-light radius of dSphs, including the MW and the Magellanic System gravitational attractions. The above is suggestive of dSphs that are DM-free and tidally shocked near their pericenters, which may provoke a significant quake in our understanding of near-field cosmology.'}\n",
      "4 {'path': '/tmp/arxiv/hep-ph.updates.on.arXiv.org/001003', 'text': 'A novel non-minimal interaction of neutral right-handed fermion and abelian gauge field in the covariant $\\\\Theta$-exact noncommutative standard model (NCSM) which is invariant under Very Special Relativity (VSR) Lorentz subgroup, opens an avenue to study the top quark pair production at linear colliders. Here the coupling denoted as $\\\\kappa$ and the noncommutative (NC) scale $\\\\Lambda$. In this work, we consider two types of analysis, one is without considering helicity basis and another, considering helicity states of the polarized and unpolarized initial and final particles. In the first case, when $\\\\kappa$ is positive and for certain values of $\\\\Lambda$, we arrived a specific threshold value of machine energy (units of GeV) $\\\\sqrt{s_0}$ ($ \\\\simeq 2.52 ~\\\\Lambda + 39$ ) may be quite useful to probe NCSM with the unpolarized beam. The statistical $\\\\chi^2$ analysis of the azimuthal anisotropy is quite possible when $\\\\kappa$ takes negative value $0 >\\\\kappa> -0.596$ which persuade a lower limit on NC scale $\\\\Lambda$ ($ 1.0\\\\, \\\\text{to} \\\\, 2.4\\\\,\\\\text{TeV}$) at $\\\\kappa_{max}=-0.296 (95\\\\%$ C.L) according to luminosity ranging from $100\\\\,fb^{-1} \\\\text{to}1000\\\\,fb^{-1}$ at machine energy $\\\\sqrt{s}=1.4\\\\,\\\\text{TeV}\\\\,\\\\text{and}\\\\, 3.0\\\\,\\\\text{TeV}$. In another case, we performed polarized beam analysis to probe NCSM in the light of following observables azimuthal anisotropy, helicity correlation, and top quark helicity left-right asymmetry. The polarization of the initial beam $\\\\{ P_{e^{-}},P_{e^{+}}\\\\} = \\\\{-0.8,0.3\\\\}( \\\\{-0.8,0.6\\\\})$ enhances the ranges of lower limit on $\\\\Lambda$, $i.e.\\\\, 1.13 \\\\, \\\\text{to} \\\\, 2.80\\\\,\\\\text{TeV}$ at $\\\\kappa_{max}$ alongside the $\\\\kappa_{max}$ enhanced into $-0.5445 \\\\,(-0.607)$ $95\\\\%$C.L accord with luminosity and $\\\\sqrt{s}$. Finally, we studied the intriguing mixing of the UV and the IR by invoking $T(2)$ VSR Lorentz subgroup symmetry on NC tensor $\\\\Theta_{\\\\mu\\\\nu}$.'}\n",
      "5 {'path': '/tmp/arxiv/hep-ph.updates.on.arXiv.org/001231', 'text': 'The new $\\\\gamma n\\\\to K^0\\\\Lambda$ data obtained from the CLAS and MAMI collaborations are analyzed by employing an effective Lagrangian method. The constructed model can describe all available experimental data in both $\\\\gamma p \\\\to K^+\\\\Lambda$ and $\\\\gamma n\\\\to K^0\\\\Lambda$ channels, simultaneously. The background part of the model is built from the appropriate intermediate states involving the nucleon, kaon, and hyperon exchanges, whereas the resonance part is constructed from the consistent interaction Lagrangians and propagators. To check the performance of the model a detailed comparison between the calculated observables and experimental data in both isospin channels is presented, from which a nice agreement can be observed. The discrepancy between the CLAS and MAMI data in the $\\\\gamma n\\\\to K^0\\\\Lambda$ channel is analyzed by utilizing three different models; M1, M2, and M3 that fit the CLAS, MAMI, and both CLAS and MAMI data sets, respectively. The effect of this discrepancy is studied by investigating the significance of individual nucleon resonances and the predicted beam-target helicity asymmetry $E$ that has been measured by the CLAS collaboration recently. It is found that the $N(1720)P_{13}$, $N(1900)P_{13}$, and $N(2060)D_{15}$ resonances are significant for improving the agreement between model calculation and data. This result is relatively stable to the choice of the model. The helicity asymmetry $E$ can be better explained by the models M1 and M3. Finally, the effect of the $N(1680)P_{11}$ narrow resonance on the cross section of both isospin channels is explored. It is found that the effect is more sensitive in the $\\\\gamma n\\\\to K^0\\\\Lambda$ channel. In this case the model M3, that fits both CLAS and MAMI data, yields a more realistic effect.'}\n",
      "6 {'path': '/tmp/arxiv/hep-ph.updates.on.arXiv.org/000125', 'text': 'Soft limits of $N$-point correlation functions, in which one wavenumber is much smaller than the others, play a special role in constraining the physics of inflation. Anisotropic sources such as a vector field during inflation generate distinct angular dependence in all these correlators. In this paper we focus on the four-point correlator (the trispectrum $T$). We adopt a parametrization motivated by models in which the inflaton $\\\\phi$ is coupled to a vector field through a $I^2 \\\\left( \\\\phi \\\\right) F^2$ interaction, namely $T_{\\\\zeta}({\\\\bf k}_1, {\\\\bf k}_2, {\\\\bf k}_3, {\\\\bf k}_4) \\\\equiv \\\\sum_n d_n [ P_n(\\\\hat{\\\\bf k}_1 \\\\cdot \\\\hat{\\\\bf k}_3) + P_n(\\\\hat{\\\\bf k}_1 \\\\cdot \\\\hat{\\\\bf k}_{12}) + P_n(\\\\hat{\\\\bf k}_3 \\\\cdot \\\\hat{\\\\bf k}_{12}) ] P_{\\\\zeta}(k_1) P_{\\\\zeta}(k_3) P_\\\\zeta(k_{12}) + (23~{\\\\rm perm})$, where $P_n$ denotes the Legendre polynomials. This shape is enhanced when the wavenumbers of the diagonals of the quadrilateral are much smaller than the sides, ${\\\\bf k}_i$. The coefficient of the isotropic part, $d_0$, is equal to $\\\\tau_{\\\\rm NL}/6$ discussed in the literature. A $I^2 \\\\left( \\\\phi \\\\right) F^2$ interaction generates $d_2 = 2 d_0$ which is, in turn, related to the quadrupole modulation parameter of the power spectrum, $g_*$, as $d_2 \\\\approx 14 |g_*| N^2$ with $N \\\\approx 60$. We show that $d_0$ and $d_2$ can be equally well-constrained: the expected $68 \\\\%$ CL error bars on these coefficients from a cosmic-variance-limited experiment measuring temperature anisotropy of the cosmic microwave background up to $\\\\ell_{\\\\rm max}=2000$ are $\\\\delta d_2 \\\\approx 4 \\\\delta d_0 = 105$. Therefore, we can reach $|g_*|=10^{-3}$ by measuring the angle-dependent trispectrum. The current upper limit on $\\\\tau_{\\\\rm NL}$ from the ${\\\\it Planck}$ temperature maps yields $|g_*|<0.02$ ($95 \\\\%$ CL).'}\n",
      "7 {'path': '/tmp/arxiv/hep-ph.updates.on.arXiv.org/000919', 'text': \"The presence of ubiquitous magnetic fields in the universe is suggested from observations of radiation and cosmic ray from galaxies or the intergalactic medium (IGM). One possible origin of cosmic magnetic fields is the magnetogenesis in the primordial universe. Such magnetic fields are called primordial magnetic fields (PMFs), and are considered to affect the evolution of matter density fluctuations and the thermal history of the IGM gas. Hence the information of PMFs is expected to be imprinted on the anisotropies of the cosmic microwave background (CMB) through the thermal Sunyaev-Zel'dovich (tSZ) effect in the IGM. In this study, given an initial power spectrum of PMFs as $P(k)\\\\propto B_{\\\\rm 1Mpc}^2 k^{n_{B}}$, we calculate dynamical and thermal evolutions of the IGM under the influence of PMFs, and compute the resultant angular power spectrum of the Compton $y$-parameter on the sky. As a result, we find that two physical processes driven by PMFs dominantly determine the power spectrum of the Compton $y$-parameter; (i) the heating due to the ambipolar diffusion effectively works to increase the temperature and the ionization fraction, and (ii) the Lorentz force drastically enhances the density contrast just after the recombination epoch. These facts result in making the tSZ angular power spectrum induced by the PMFs more remarkable at $\\\\ell >10^4$ than that by galaxy clusters even with $B_{\\\\rm 1Mpc}=0.1$ nG and $n_{B}=-1.0$ because the contribution from galaxy clusters decreases with increasing $\\\\ell$. The measurement of the tSZ angular power spectrum on high $\\\\ell$ modes can provide the stringent constraint on PMFs.\"}\n",
      "8 {'path': '/tmp/arxiv/hep-ph.updates.on.arXiv.org/000317', 'text': \"Using the Fenchel-Eggleston theorem for convex hulls (an extension of the Caratheodory theorem), we prove that any likelihood can be maximized by either a dark matter 1- speed distribution $F(v)$ in Earth's frame or 2- Galactic velocity distribution $f^{\\\\rm gal}(\\\\vec{u})$, consisting of a sum of delta functions. The former case applies only to time-averaged rate measurements and the maximum number of delta functions is $({\\\\mathcal N}-1)$, where ${\\\\mathcal N}$ is the total number of data entries. The second case applies to any harmonic expansion coefficient of the time-dependent rate and the maximum number of terms is ${\\\\mathcal N}$. Using time-averaged rates, the aforementioned form of $F(v)$ results in a piecewise constant unmodulated halo function $\\\\tilde\\\\eta^0_{BF}(v_{\\\\rm min})$ (which is an integral of the speed distribution) with at most $({\\\\mathcal N}-1)$ downward steps. The authors had previously proven this result for likelihoods comprised of at least one extended likelihood, and found the best-fit halo function to be unique. This uniqueness, however, cannot be guaranteed in the more general analysis applied to arbitrary likelihoods. Thus we introduce a method for determining whether there exists a unique best-fit halo function, and provide a procedure for constructing either a pointwise confidence band, if the best-fit halo function is unique, or a degeneracy band, if it is not. Using measurements of modulation amplitudes, the aforementioned form of $f^{\\\\rm gal}(\\\\vec{u})$, which is a sum of Galactic streams, yields a periodic time-dependent halo function $\\\\tilde\\\\eta_{BF}(v_{\\\\rm min}, t)$ which at any fixed time is a piecewise constant function of $v_{\\\\rm min}$ with at most ${\\\\mathcal N}$ downward steps. In this case, we explain how to construct pointwise confidence and degeneracy bands from the time-averaged halo function. Finally, we show that requiring an isotropic ...\"}\n",
      "9 {'path': '/tmp/arxiv/hep-ph.updates.on.arXiv.org/000773', 'text': 'We present a global fit to all data on the suppression of high energy jets and high energy hadrons in the most central heavy ion collisions at the LHC for two different collision energies, within a hybrid strong/weak coupling quenching model. Even though the measured suppression factors for hadrons and jets differ significantly from one another and appear to asymptote to different values in the high energy limit, we obtain a simultaneous description of all these data after constraining the value of a single model parameter. We use our model to investigate the origin of the difference between the observed suppression of jets and hadrons and relate it, quantitatively, to the observed modification of the jet fragmentation function in jets that have been modified by passage through the medium produced in heavy ion collisions. In particular, the observed increase in the fraction of hard fragments in medium-modified jets, which indicates that jets with the fewest hardest fragments lose the least energy, corresponds quantitatively to the observed difference between the suppression of hadrons and jets. We argue that a harder fragmentation pattern for jets with a given energy after quenching is a generic feature of any mechanism for the interaction between jets and the medium that they traverse that yields a larger suppression for wider jets. We also compare the results of our global fit to LHC data to measurements of the suppression of high energy hadrons in RHIC collisions, and find that with its parameter chosen to fit the LHC data our model is inconsistent with the RHIC data at the $3\\\\sigma$ level, suggesting that hard probes interact more strongly with the less hot quark-gluon plasma produced at RHIC.'}\n",
      "10 {'path': '/tmp/arxiv/hep-ph.updates.on.arXiv.org/000541', 'text': 'We scrutinize corrections to tribimaximal (TBM), bimaximal (BM) and democratic (DC) mixing matrices for explaining recent global fit neutrino mixing data. These corrections are parameterized in terms of small orthogonal rotations (R) with corresponding modified PMNS matrices of the forms \\\\big($R_{ij}^l\\\\cdot U,~U\\\\cdot R_{ij}^r,~U \\\\cdot R_{ij}^r \\\\cdot R_{kl}^r,~R_{ij}^l \\\\cdot R_{kl}^l \\\\cdot U$\\\\big ) where $R_{ij}^{l, r}$ is rotation in ij sector and U is any one of these special matrices. We showed that for perturbative schemes dictated by single rotation, only \\\\big($ R_{12}^l\\\\cdot U_{BM},~R_{13}^l\\\\cdot U_{BM},~U_{TBM}\\\\cdot R_{13}^r$ \\\\big ) can fit the mixing data at $3\\\\sigma$ level. However for $R_{ij}^l\\\\cdot R_{kl}^l\\\\cdot U$ type rotations, only \\\\big ($R_{23}^l\\\\cdot R_{13}^l \\\\cdot U_{DC} $\\\\big ) is successful to fit all neutrino mixing angles within $1\\\\sigma$ range. For $U\\\\cdot R_{ij}^r\\\\cdot R_{kl}^r$ perturbative scheme, only \\\\big($U_{BM} \\\\cdot R_{12}^r\\\\cdot R_{13}^r$,~$U_{DC} \\\\cdot R_{12}^r\\\\cdot R_{23}^r$,~$U_{TBM} \\\\cdot R_{12}^r\\\\cdot R_{13}^r$\\\\big ) are consistent at $1\\\\sigma$ level. The remaining double rotation cases are either excluded at 3$\\\\sigma$ level or successful in producing mixing angles only at $2\\\\sigma-3\\\\sigma$ level. We also updated our previous analysis on PMNS matrices of the form \\\\big($R_{ij}\\\\cdot U \\\\cdot R_{kl}$\\\\big ) with recent mixing data. We showed that the results modifies substantially with fitting accuracy level decreases for all of the permitted cases except \\\\big($R_{12}\\\\cdot U_{BM}\\\\cdot R_{13}$, $R_{23}\\\\cdot U_{TBM}\\\\cdot R_{13}$ and $R_{13}\\\\cdot U_{TBM} \\\\cdot R_{13}$\\\\big ) in this rotation scheme.'}\n"
     ]
    }
   ],
   "source": [
    "def print_docs_from_index(index_name, client, max_docs):\n",
    "\n",
    "    print(f\"===================\")\n",
    "    info = client.cat.count(index=index_name, format = \"json\")[0]\n",
    "    print(f\"Index: {index_name} with {info['count']} documents.\")\n",
    "    print()\n",
    "\n",
    "    res = client.search(index=index_name, size = max_docs, query= {'match_all' : {}})\n",
    "\n",
    "    for doc in res['hits']['hits']:\n",
    "        print (doc['_id'], doc['_source'])\n",
    "\n",
    "print_docs_from_index('arxiv', Elasticsearch(\"http://localhost:9200\", request_timeout=1000), max_docs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "crk-hI4I0EyC"
   },
   "source": [
    "---\n",
    "\n",
    "**Exercise 5: (may take time..)**\n",
    "\n",
    "Can you find duplicate documents in the corpus provided?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "CAIM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "nav_menu": {
    "height": "81px",
    "width": "252px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
